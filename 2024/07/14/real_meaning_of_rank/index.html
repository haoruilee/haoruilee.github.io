<!DOCTYPE HTML>
<html>

<head>
	<link rel="bookmark"  type="image/x-icon"  href="/img/logo_miccall.webp"/>
	<link rel="shortcut icon" href="/img/logo_miccall.webp">
	
			    <title>
    Haorui Li
    </title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="/css/mic_main.css" />
    <link rel="stylesheet" href="/css/dropdownMenu.css" />
    <meta name="keywords" content="haoruilee" />
    
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css" />
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('https://i.dawnlab.me/e6a2105d63e4438f288e4ae4d97f53f2.webp') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

			    
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script async type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


    <script src="https://cdnjs.loli.net/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
    <script src="https://cdnjs.loli.net/ajax/libs/jquery.scrollex/0.2.1/jquery.scrollex.min.js"></script>
    <script src="https://cdnjs.loli.net/ajax/libs/skel/3.0.1/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
<meta name="generator" content="Hexo 7.2.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_coy.css" />
<link rel="stylesheet" href="/css/typo.css" />
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="mailto:haoruili@seu.edu.cn" class="logo">Mail Me for offical  or Chat with me for fun</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special" >
            <ul class="menu links" >
			<!-- Homepage  主页  --> 
			<li >
	            <a href="/" rel="nofollow">Home</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <!-- archives  归档   --> 
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/life/" title="CV">
		                CV
		            </a>
		        </li>
		        
		        <li>
		            <a href="/tag/" title="Contact">
		                Contact
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
                    
                    <li>
                        <a title="github" href="https://github.com/haoruilee" target="_blank" rel="noopener">
                            <i class="icon fa fa-github"></i>
                        </a>
                    </li>
                    
			</ul>
</nav>

        <div id="main" >
            <div class ="post_page_title_img" style="height: 25rem;background-image: url(https://i.dawnlab.me/cc0041adbacfae3c0fea386293404ecf.png);background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;" >
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2 >Real Meaning of Rank</h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
                <h1 id="The-True-Meaning-of-Determinants-and-Matrix-Rank"><a href="#The-True-Meaning-of-Determinants-and-Matrix-Rank" class="headerlink" title="The True Meaning of Determinants and Matrix Rank"></a>The True Meaning of Determinants and Matrix Rank</h1><p>Here, we first discuss a mathematical question that has long puzzled students of engineering and even physics: What is the real meaning of area, and how is it generalized to higher dimensions?</p>
<h2 id="Area-a-Type-of-Mapping"><a href="#Area-a-Type-of-Mapping" class="headerlink" title="Area, a Type of Mapping"></a>Area, a Type of Mapping</h2><p>You might say, area is just length multiplied by width, but that’s not entirely true. Let’s clarify that the area we’re discussing here is the basic unit of Euclidean geometric area: the area of a parallelogram. Geometrically, the area of a parallelogram is defined as the product of the lengths of two adjacent sides and the sine of the angle between them.</p>
<p>However, to deal with more general situations and higher-dimensional mathematical problems, we need to extend the definition of area. Note the following fact:</p>
<p>Area is a scalar, derived from two vectors (forming its adjacent sides). Therefore, we can view the area as a mapping:</p>
<p>$$<br>\varphi: \mathfrak{X}(M) \times \mathfrak{X}(M) \rightarrow \mathfrak{F}(M), V \times V \mapsto f<br>$$</p>
<p>Where ( V ) is a vector, ( V \times V ) represents an ordered pair of vectors, and ( f ) is the value of the area.</p>
<p>Next, we will show that this mapping is a linear one.</p>
<p>Starting with the simplest example: If the first vector is (1,0) and the second vector is (0,1), meaning the two vectors are unit vectors on the X and Y axes respectively, then the parallelogram formed by these two vectors is a square. According to the definition, its area is length multiplied by width &#x3D; 1*1 &#x3D; 1.</p>
<p><img src="https://chatgpt5.oss-rg-china-mainland.aliyuncs.com/image/real_meaning_of_rank/image/real_meaning_of_rank/1720928890761.png" alt="1720928890761"></p>
<p>Therefore:</p>
<p>$$<br>\varphi((1,0),(0,1))&#x3D;1<br>$$</p>
<p>If we “scale” the first vector by a factor of ( a ), the area will correspondingly be ( a ) times the original; if we “scale” the second vector by a factor of ( b ), the area will also become ( b ) times the original. If both vectors are scaled, it is obvious that the area will become ( ab ) times the original area. This shows that the area mapping is linear with respect to the scalar multiplication of its two operands (vectors), as follows:</p>
<p>$$<br>\begin{aligned}<br>&amp; \varphi(a(1,0),(0,1))&#x3D;a \varphi((1,0),(0,1)) \<br>&amp; \varphi((1,0), b(0,1))&#x3D;b \varphi((1,0),(0,1))<br>\end{aligned} \Rightarrow \varphi(a(1,0), b(0,1))&#x3D;a b \varphi((1,0),(0,1))&#x3D;a b<br>$$</p>
<p>Finally, we must show that the area mapping is also linear with respect to the vector addition of its operands. Since the operation of vector addition is itself linear, its area mapping should naturally be a linear mapping. Here we intend to illustrate the consequences of the linearity of the mapping’s addition through several practical examples.</p>
<p>It is evident that the parallelogram formed by two collinear vectors is still a line, and therefore the area is zero:</p>
<p><img src="https://chatgpt5.oss-rg-china-mainland.aliyuncs.com/image/real_meaning_of_rank/image/real_meaning_of_rank/1720928989597.png" alt="1720928989597"></p>
<p>Which means:</p>
<p>$$<br>\varphi(a,a)&#x3D;0<br>$$</p>
<p>Assuming the area mapping is a linear mapping concerning vector addition, we have:</p>
<p>$$<br>\begin{aligned}<br>\varphi((1,0)+(0,1),(1,0)+(0,1))&#x3D;0 &amp; &#x3D;\varphi((1,0),(0,1))+\varphi((1,0),(1,0))+\varphi((0,1),(0,1))+\varphi((0,1),(1,0)) \<br>&amp; &#x3D;\varphi((1,0),(0,1))+\varphi((0,1),(1,0))<br>\end{aligned}<br>$$</p>
<p>Thus we get:</p>
<p>$$<br>\varphi((1,0),(0,1)) &#x3D; -\varphi((0,1),(1,0))<br>$$</p>
<p>That is to say, exchanging the order of mutually perpendicular operand vectors changes the sign of the area mapping. Which is positive and which is negative depends on the definition. Generally, we place the unit vector on the X-axis first and the unit vector on the Y-axis second, and take the area of the parallelogram formed from the X-axis to the Y-axis as positive.</p>
<p>Thus, we introduce the right-hand rule. Note that the right-hand rule is only valid in three-dimensional space. If we take the positive direction of the X-axis as the head and the positive direction of the Y-axis as the tail, the right-hand rule tells us that the outward direction from the paper is the positive direction of the area; if reversed, the inward direction from the paper is the positive direction of the area, opposite to the defined positive direction, and the sign is negative. The geometric meaning of the area’s sign then becomes apparent.</p>
<p>It is not difficult to see that the so-called area is the determinant of a 2x2 matrix:</p>
<p>$$<br>\left|\begin{array}{ll}<br>a &amp; b \<br>c &amp; d<br>\end{array}\right|&#x3D;\left|\begin{array}{ll}<br>a &amp; c \<br>b &amp; d<br>\end{array}\right|&#x3D;a d-b c<br>$$</p>
<p>As shown below:</p>
<p><img src="https://chatgpt5.oss-rg-china-mainland.aliyuncs.com/image/real_meaning_of_rank/image/real_meaning_of_rank/1720929547070.png" alt="1720929547070"></p>
<p>Where the first row is our first row vector ( (a, b) ); the second row is the second row vector ( (c, d) ). Or the first column is the first column vector ( (a, b)^T ), and the second column is the second column vector ( (c, d)^T ). This depends on whether we write the vector as a row vector (the former) or a column vector (the latter).</p>
<p>From this, we can easily see that the value of the determinant is independent of whether the vectors are written as row vectors or column vectors. This is why we say that rows and columns are equal when calculating determinants. Additionally, note that from the above analysis, exchanging the order of vectors results in the area value taking a negative sign. This explains why exchanging row or column vectors in a determinant changes the sign. Other properties of determinant calculation are also reflected in the linearity of the area mapping.</p>
<p>Thus, we see that the determinant is a generalization of “area.” It represents the volume of an N-dimensional generalized parallelepiped spanned by N vectors under a given set of bases. This is the essential meaning of the determinant.</p>
<h2 id="Generalizing-Determinants"><a href="#Generalizing-Determinants" class="headerlink" title="Generalizing Determinants"></a>Generalizing Determinants</h2><p>From the above, we can easily extend to the calculation of three-dimensional volume:</p>
<p>$$<br>V(\vec{a}, \vec{b}, \vec{c})&#x3D;\left|\begin{array}{c}<br>\vec{a} \<br>\vec{b} \<br>\vec{c}<br>\end{array}\right|&#x3D;\left|\begin{array}{lll}<br>\vec{a} &amp; \vec{b} &amp; \vec{c}<br>\end{array}\right|<br>$$</p>
<p>Note that the definition of the determinant involves taking the product of elements from different rows and columns, and the sign is related to the so-called parity. Parity has a geometric meaning: after defining a positive direction (such as the sequence 1, 2, 3, 4, 5…N as positive), swapping any pair of numbers changes the sign. We have seen this property in the area function above. In fact, volume and higher-dimensional generalized volumes also have a concept of positive direction, although it is challenging to illustrate using the right-hand rule and cross products. The limitation of the right-hand rule is one motivation for expressing higher-dimensional areas as determinants.</p>
<p>This property, where swapping any pair of indices (operands) changes the sign, is called antisymmetric. The reason for taking products of elements from different rows and columns is that if any two elements are from the same row (or column), swapping their column indices leaves the product unchanged but changes the sign, making the product zero, which does not contribute to the determinant’s value.</p>
<p>The definition of the determinant is complicated because of the antisymmetry of the area mapping. In fact, the area mapping is a 2-form. Extending a 2-form to any R-form, we see that the form and the determinant of an R x R matrix are identical.</p>
<p>From the above, we can see that a 2-form represents area in a plane; a 3-form represents volume in three-dimensional space; a 4-form represents hypervolume in four-dimensional space, and so on. In reality, by writing these vectors as matrices under a given set of bases (which must be square matrices), the matrix’s determinant corresponds to the area (volume). The proof of this generalization can be found in any specialized textbook on linear algebra (or self-verified if not).</p>
<h2 id="The-Geometric-Meaning-of-Linear-Independence"><a href="#The-Geometric-Meaning-of-Linear-Independence" class="headerlink" title="The Geometric Meaning of Linear Independence"></a>The Geometric Meaning of Linear Independence</h2><p>Let N be the dimension of space, and given a set of vectors, what does it mean for them to be linearly independent? We will explain that the essence of linear independence of a set of vectors is whether the volume of the generalized parallelepiped spanned by them is zero.</p>
<p>We start again from the simplest two-dimensional space. If two vectors in a two-dimensional space are linearly dependent, one is collinear with the other, meaning the parallelogram they span has an area of zero. Conversely, if they are linearly independent, they are not collinear, so the area is not zero.</p>
<p>Similarly, if three vectors in three-dimensional space are linearly independent, they do not lie in the same plane, so the volume of the parallelepiped they span is not zero.</p>
<p>Furthermore, we know that in a two-dimensional space, if three vectors are given, they must lie in the same plane (a two-dimensional space cannot have a “volume”), so they must be linearly dependent. Therefore, we can understand why in an N-dimensional space, any set of M vectors (where M &gt; N) must be linearly dependent: because a generalized parallelepiped of dimension greater than the space’s dimension does not exist.</p>
<p>Thus, we obtain a one-to-one correspondence:</p>
<p>N linearly independent vectors &#x3D;&#x3D; The volume of the N-dimensional body they span is not zero.</p>
<p>Conversely, if N vectors are linearly dependent, the volume of the N-dimensional body they span is zero.</p>
<p>For example, the parallelogram formed by a pair of collinear vectors degenerates into a line, and its area is obviously zero; the parallelepiped formed by a set of coplanar vectors degenerates into a plane, and its volume is obviously zero.</p>
<p>Since we already know the relationship between the determinant and the area, we conclude:</p>
<p>The determinant of a matrix composed of linearly independent vectors is not zero; the determinant of a matrix composed of linearly dependent vectors must be zero.</p>
<h2 id="Determinants-and-Matrix-Inverses"><a href="#Determinants-and-Matrix-Inverses" class="headerlink" title="Determinants and Matrix Inverses"></a>Determinants and Matrix Inverses</h2><p>We know that a matrix with a determinant of zero is non-invertible; a matrix with a non-zero determinant is invertible. One might ask, how is the determinant, representing area, related to the invertibility of linear transformations?</p>
<p>When we understand the geometric meaning of linear transformations, it becomes clear. We state the following:</p>
<p>Let the matrix of the linear transformation be ( A ).</p>
<p>If we write a set of linearly independent vectors in space as column vectors, the volume of the N-dimensional body they span is not zero. According to the above analysis, its value is given by the determinant. After the vectors are transformed by the linear transformation ( A ), the new vectors are:</p>
<p>$$<br>\vec{a}_i^{\prime}&#x3D;A \vec{a}_i, i \in{1, \ldots, n}<br>$$</p>
<p>Note that ( A ) is an ( N \times N ) matrix, and the vectors are column vectors.</p>
<p>Before the transformation, the volume of the N-dimensional body is:</p>
<p>$$<br>V &#x3D; \left|  \vec{a}_1, \vec{a}_2, …, \vec{a}_n \right|<br>$$</p>
<p>After the transformation, the volume of the N-dimensional body is (note that the second equation actually explains the geometric meaning of matrix multiplication, i.e., the multiplication of an ( N \times N ) matrix ( A ) with another ( N \times N ) matrix composed of ( N ) column vectors):</p>
<p>$$<br>\begin{aligned}<br>&amp; V^{\prime}&#x3D;\left|\begin{array}{llll}<br>\vec{a}_1^{\prime} &amp; \vec{a}_2^{\prime} &amp; \ldots &amp; \vec{a}_n^{\prime}<br>\end{array}\right|&#x3D;\left|A \cdot\left(\begin{array}{llll}<br>\vec{a}_1 &amp; \vec{a}_2 &amp; \ldots &amp; \vec{a}_n<br>\end{array}\right)\right| \<br>&amp; &#x3D;|A| \left\lvert, \begin{array}{llll}<br>\vec{a}_1 &amp; \vec{a}_2 &amp; \cdots &amp; \vec{a}_n|&#x3D;| A \mid V<br>\end{array}\right. \<br>&amp;<br>\end{aligned}<br>$$</p>
<p>If the determinant of ( A ) is not zero, it means that after the transformation, the volume of the N-dimensional body is not null. Combined with the nature of linear independence and volume, we can say:</p>
<p>If the determinant of ( A ) is not zero, then ( A ) can map a set of linearly independent vectors to another set of linearly independent vectors; ( A ) is invertible (a one-to-one mapping, faithful mapping, kernel is {0}).</p>
<p>If the determinant of ( A ) is zero, then ( A ) will map a set of linearly independent vectors to a set of linearly dependent vectors; ( A ) is not invertible (non-faithful mapping, kernel is not {0}). We can study its cosets.</p>
<p>If the determinant of ( A ) is negative, ( A ) will change the orientation of the original N-dimensional body’s volume.</p>
<p>From linear independence to linear dependence, some information is lost (e.g., collapsing into collinearity or coplanarity), making the transformation obviously non-invertible. The linear independence of vectors and the volume of the N-dimensional body they span are directly related, and this volume value is related to the determinant of ( A ). Therefore, we establish a geometric relationship between the determinant of ( A ) and its invertibility.</p>
<p>For example, suppose ( A ) is a three-dimensional matrix. If there is a set of three linearly independent vectors before the mapping, we know that the volume they span is not zero. After the mapping, the new vectors can still span a parallelepiped, and the volume of this parallelepiped is the original volume multiplied by the determinant of ( A ).</p>
<p>Obviously, if the determinant of ( A ) is zero, the volume of the new “parallelepiped” after the transformation will inevitably be zero. According to the previous conclusion, the new set of vectors after the transformation is linearly dependent.</p>
<p>Conclusion:</p>
<p>Whether the determinant of the linear transformation ( A ) is zero represents the fidelity of its mapping, i.e., whether it can map a set of linearly independent vectors to another set of linearly independent vectors.</p>
<h2 id="Matrix-Rank"><a href="#Matrix-Rank" class="headerlink" title="Matrix Rank"></a>Matrix Rank</h2><p>Sometimes, although ( A ) cannot maintain the linear independence of the largest set of vectors in space, it can maintain the linear independence of a smaller set of vectors. This number is often less than the dimension of ( A ) (or the dimension of the linear space), and this number is called the rank of the linear transformation ( A ).</p>
<p>For example, a 3x3 matrix ( A ) with a rank of 2. Because the rank is less than 3, any three-dimensional parallelepiped after its transformation has a volume of zero (degenerates into a plane); but there exists a plane with a non-zero area that can still be transformed into a plane with a non-zero area.</p>
<p>The so-called rank of a linear transformation is simply the maximum dimension of the geometric shapes that can maintain non-zero volume after the transformation.</p>
<p>Understanding the geometric meaning of rank, determinant, and invertibility, we can easily construct some linear transformations ( A ) that either preserve all geometric bodies or compress specific-dimensional geometric bodies into lower-dimensional geometric bodies. Isn’t this the so-called “dimensionality reduction strike”? Therefore, the ultimate move in the “Three-Body Problem” is essentially a linear transformation with a determinant of zero and a rank one less than the dimension.</p>
<p>The extension to higher dimensions is left to the reader; also, the proof of the linearity of the area function is left to the reader to verify strictly.</p>
<p>Bo Zheng,<br>UCB, 2011<br>From <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/19609459">https://zhuanlan.zhihu.com/p/19609459</a></p>
<h1 id="My-comments"><a href="#My-comments" class="headerlink" title="My comments"></a>My comments</h1><p>Lets think some reality examples to use bozheng’s theory.</p>
<h2 id="Matrix-Decomposition-and-Transformations-into-n-Dimensional-Subspaces"><a href="#Matrix-Decomposition-and-Transformations-into-n-Dimensional-Subspaces" class="headerlink" title="Matrix Decomposition and Transformations into n-Dimensional Subspaces"></a>Matrix Decomposition and Transformations into n-Dimensional Subspaces</h2><p>Building upon our discussion of determinants and matrix rank, we now delve into the fascinating realm of matrix decomposition. Understanding how matrices of different ranks can be decomposed not only illuminates their structural properties but also provides deep insights into how these matrices transform spaces into various dimensional subspaces.</p>
<h3 id="Matrix-Decomposition-Based-on-Rank"><a href="#Matrix-Decomposition-Based-on-Rank" class="headerlink" title="Matrix Decomposition Based on Rank"></a>Matrix Decomposition Based on Rank</h3><p>The <strong>rank</strong> of a matrix fundamentally dictates how it can be decomposed into simpler, rank-1 components. This decomposition is pivotal for simplifying complex linear transformations and for applications across engineering, physics, and data science.</p>
<h4 id="1-Rank-0-The-Zero-Matrix"><a href="#1-Rank-0-The-Zero-Matrix" class="headerlink" title="1. Rank 0: The Zero Matrix"></a>1. <strong>Rank 0: The Zero Matrix</strong></h4><ul>
<li><p><strong>Definition</strong>: A matrix $A$ has rank 0 if and only if all its entries are zero.</p>
</li>
<li><p><strong>Decomposition</strong>:<br>$$<br>A &#x3D; 0 &#x3D; \mathbf{0} \cdot \mathbf{0}^T<br>$$</p>
</li>
<li><p><strong>Geometric Interpretation</strong>: The zero matrix maps every vector to the zero vector, effectively collapsing the entire space into a single point.</p>
</li>
</ul>
<h4 id="2-Rank-1-Outer-Product-of-Two-Vectors"><a href="#2-Rank-1-Outer-Product-of-Two-Vectors" class="headerlink" title="2. Rank 1: Outer Product of Two Vectors"></a>2. <strong>Rank 1: Outer Product of Two Vectors</strong></h4><ul>
<li><p><strong>Definition</strong>: A matrix $A$ has rank 1 if it can be expressed as the outer product of two non-zero vectors.</p>
</li>
<li><p><strong>Decomposition</strong>:<br>$$<br>A &#x3D; \mathbf{u} \mathbf{v}^T<br>$$<br>where $\mathbf{u}, \mathbf{v} \in \mathbb{R}^3$.</p>
</li>
<li><p><strong>Geometric Interpretation</strong>:</p>
<ul>
<li><strong>Column Space</strong>: All columns of $A$ are scalar multiples of $\mathbf{u}$, lying along a single line in $\mathbb{R}^3$.</li>
<li><strong>Row Space</strong>: All rows of $A$ are scalar multiples of $\mathbf{v}^T$, also lying along a single line.</li>
<li><strong>Transformation</strong>: $A$ maps any vector $\mathbf{x}$ to a vector in the direction of $\mathbf{u}$, scaled by the projection of $\mathbf{x}$ onto $\mathbf{v}$.</li>
</ul>
</li>
<li><p><strong>Example</strong>:<br>$$<br>A &#x3D; \begin{pmatrix} 2 \ 4 \ 6 \end{pmatrix} \begin{pmatrix} 1 &amp; 0 &amp; -1 \end{pmatrix} &#x3D; \begin{pmatrix} 2 &amp; 0 &amp; -2 \ 4 &amp; 0 &amp; -4 \ 6 &amp; 0 &amp; -6 \end{pmatrix}<br>$$<br>This matrix has rank 1, as all columns are multiples of $\begin{pmatrix} 2 \ 4 \ 6 \end{pmatrix}$.</p>
</li>
</ul>
<h4 id="3-Rank-2-Sum-of-Two-Rank-1-Matrices"><a href="#3-Rank-2-Sum-of-Two-Rank-1-Matrices" class="headerlink" title="3. Rank 2: Sum of Two Rank-1 Matrices"></a>3. <strong>Rank 2: Sum of Two Rank-1 Matrices</strong></h4><ul>
<li><p><strong>Definition</strong>: A matrix $A$ has rank 2 if it can be expressed as the sum of two rank-1 matrices.</p>
</li>
<li><p><strong>Decomposition</strong>:</p>
<p>$$<br>A &#x3D; \mathbf{a}_1 \mathbf{b}_1^T + \mathbf{a}_2 \mathbf{b}_2^T<br>$$</p>
<p>where $\mathbf{a}_1, \mathbf{a}_2 \in \mathbb{R}^3$ are linearly independent, and $\mathbf{b}_1, \mathbf{b}_2 \in \mathbb{R}^3$.</p>
</li>
<li><p><strong>Geometric Interpretation</strong>:</p>
<ul>
<li><strong>Column Space</strong>: Spanned by the two vectors $\mathbf{a}_1$ and $\mathbf{a}_2$, forming a plane in $\mathbb{R}^3$.</li>
<li><strong>Row Space</strong>: Spanned by the two vectors $\mathbf{b}_1$ and $\mathbf{b}_2$, also forming a plane.</li>
<li><strong>Transformation</strong>: $A$ maps vectors in $\mathbb{R}^3$ onto a two-dimensional subspace, effectively “flattening” the space onto a plane.</li>
</ul>
</li>
<li><p><strong>Example</strong>:</p>
<p>$$<br>A &#x3D; \begin{pmatrix} 1 \ 2 \ 3 \end{pmatrix} \begin{pmatrix} 4 &amp; 5 &amp; 6 \end{pmatrix} + \begin{pmatrix} 7 \ 8 \ 9 \end{pmatrix} \begin{pmatrix} 10 &amp; 11 &amp; 12 \end{pmatrix} &#x3D; \begin{pmatrix} 74 &amp; 82 &amp; 90 \ 88 &amp; 98 &amp; 108 \ 102 &amp; 114 &amp; 126 \end{pmatrix}<br>$$<br>This matrix has rank 2, as it is the sum of two rank-1 matrices with linearly independent column vectors.</p>
</li>
</ul>
<h4 id="4-Rank-3-Full-Rank-Matrix"><a href="#4-Rank-3-Full-Rank-Matrix" class="headerlink" title="4. Rank 3: Full-Rank Matrix"></a>4. <strong>Rank 3: Full-Rank Matrix</strong></h4><ul>
<li><p><strong>Definition</strong>: A matrix $A$ has rank 3 if it cannot be expressed as the sum of fewer than three rank-1 matrices.</p>
</li>
<li><p><strong>Decomposition</strong>:<br>$$<br>A &#x3D; \mathbf{u}_1 \mathbf{v}_1^T + \mathbf{u}_2 \mathbf{v}_2^T + \mathbf{u}_3 \mathbf{v}_3^T<br>$$<br>where $\mathbf{u}_1, \mathbf{u}_2, \mathbf{u}_3 \in \mathbb{R}^3$ are linearly independent, and $\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3 \in \mathbb{R}^3$.</p>
</li>
<li><p><strong>Geometric Interpretation</strong>:</p>
<ul>
<li><strong>Column Space</strong>: Spanned by three linearly independent vectors, filling the entire $\mathbb{R}^3$ space.</li>
<li><strong>Row Space</strong>: Similarly, spanned by three linearly independent vectors.</li>
<li><strong>Transformation</strong>: $A$ is invertible and maps $\mathbb{R}^3$ onto itself without any loss of dimensionality.</li>
</ul>
</li>
<li><p><strong>Example</strong>:<br>$$<br>A &#x3D; \mathbf{u}_1 \mathbf{v}_1^T + \mathbf{u}_2 \mathbf{v}_2^T + \mathbf{u}_3 \mathbf{v}_3^T<br>$$<br>Where $\mathbf{u}_1, \mathbf{u}_2, \mathbf{u}_3$ and $\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3$ are all linearly independent vectors in $\mathbb{R}^3$.</p>
</li>
</ul>
<h3 id="Transformations-into-n-Dimensional-Subspaces"><a href="#Transformations-into-n-Dimensional-Subspaces" class="headerlink" title="Transformations into n-Dimensional Subspaces"></a>Transformations into n-Dimensional Subspaces</h3><p>Matrix decomposition provides a powerful framework for understanding how linear transformations interact with the geometry of space. By decomposing a matrix based on its rank, we can visualize how it projects, rotates, scales, or otherwise transforms vectors within $\mathbb{R}^n$.</p>
<h4 id="1-Rank-1-Transformations-Projection-onto-a-Line"><a href="#1-Rank-1-Transformations-Projection-onto-a-Line" class="headerlink" title="1. Rank 1 Transformations: Projection onto a Line"></a>1. <strong>Rank 1 Transformations: Projection onto a Line</strong></h4><ul>
<li><p><strong>Description</strong>: A rank 1 matrix maps all vectors onto a single line in $\mathbb{R}^n$.</p>
</li>
<li><p><strong>Geometric Action</strong>: Every input vector is scaled and directed along the vector $\mathbf{u}$, effectively projecting the entire space onto the line spanned by $\mathbf{u}$.</p>
</li>
<li><p><strong>Visualization</strong>: Imagine shining a light perpendicular to the line spanned by $\mathbf{u}$; all shadows (images) of vectors under the transformation $A &#x3D; \mathbf{u} \mathbf{v}^T$ fall onto that line.</p>
</li>
</ul>
<h4 id="2-Rank-2-Transformations-Projection-onto-a-Plane"><a href="#2-Rank-2-Transformations-Projection-onto-a-Plane" class="headerlink" title="2. Rank 2 Transformations: Projection onto a Plane"></a>2. <strong>Rank 2 Transformations: Projection onto a Plane</strong></h4><ul>
<li><p><strong>Description</strong>: A rank 2 matrix maps all vectors onto a two-dimensional plane within $\mathbb{R}^n$.</p>
</li>
<li><p><strong>Geometric Action</strong>: The transformation combines projections along two independent directions, preserving the planar structure while collapsing higher-dimensional information.</p>
</li>
<li><p><strong>Visualization</strong>: Picture projecting vectors onto a sheet of paper. The transformation preserves two-dimensional relationships while losing information in the perpendicular direction.</p>
</li>
</ul>
<h4 id="3-Rank-k-Transformations-Projection-onto-a-k-Dimensional-Subspace"><a href="#3-Rank-k-Transformations-Projection-onto-a-k-Dimensional-Subspace" class="headerlink" title="3. Rank $k$ Transformations: Projection onto a $k$-Dimensional Subspace"></a>3. <strong>Rank $k$ Transformations: Projection onto a $k$-Dimensional Subspace</strong></h4><ul>
<li><p><strong>Description</strong>: In general, a rank $k$ matrix maps vectors onto a $k$-dimensional subspace of $\mathbb{R}^n$.</p>
</li>
<li><p><strong>Geometric Action</strong>: The transformation maintains the structure within the $k$-dimensional subspace while eliminating components orthogonal to it.</p>
</li>
<li><p><strong>Visualization</strong>: For $k &#x3D; 3$, the transformation preserves three-dimensional volume. For higher $k$, it preserves higher-dimensional analogs of volume, such as hypervolumes.</p>
</li>
</ul>
<h3 id="Singular-Value-Decomposition-SVD-and-Rank"><a href="#Singular-Value-Decomposition-SVD-and-Rank" class="headerlink" title="Singular Value Decomposition (SVD) and Rank"></a>Singular Value Decomposition (SVD) and Rank</h3><p>One of the most powerful tools for matrix decomposition is the <strong>Singular Value Decomposition (SVD)</strong>. SVD provides a canonical form that reveals the rank and intrinsic geometric properties of a matrix.</p>
<h4 id="Singular-Value-Decomposition"><a href="#Singular-Value-Decomposition" class="headerlink" title="Singular Value Decomposition"></a><strong>Singular Value Decomposition</strong></h4><ul>
<li><p><strong>Definition</strong>:<br>$$<br>A &#x3D; U \Sigma V^T<br>$$<br>where:</p>
<ul>
<li>$U$ is an $m \times m$ orthogonal matrix.</li>
<li>$\Sigma$ is an $m \times n$ diagonal matrix with non-negative real numbers on the diagonal (singular values).</li>
<li>$V$ is an $n \times n$ orthogonal matrix.</li>
</ul>
</li>
<li><p><strong>Relation to Rank</strong>:</p>
<ul>
<li>The number of non-zero singular values in $\Sigma$ corresponds to the rank of $A$.</li>
<li>Each non-zero singular value represents a dimension in the column and row spaces.</li>
</ul>
</li>
<li><p><strong>Geometric Interpretation</strong>:</p>
<ul>
<li><strong>Columns of $V$</strong>: Represent the directions in the input space that are stretched or compressed.</li>
<li><strong>Columns of $U$</strong>: Represent the directions in the output space corresponding to these stretched or compressed inputs.</li>
<li><strong>Singular Values</strong>: Indicate the magnitude of stretching or compression along each corresponding direction.</li>
</ul>
</li>
</ul>
<h4 id="Example-of-SVD"><a href="#Example-of-SVD" class="headerlink" title="Example of SVD"></a><strong>Example of SVD</strong></h4><p>Consider a rank 2 matrix $A$:<br>$$<br>A &#x3D; \mathbf{a}_1 \mathbf{b}_1^T + \mathbf{a}_2 \mathbf{b}_2^T<br>$$<br>Its SVD can be written as:<br>$$<br>A &#x3D; U \Sigma V^T &#x3D; \sigma_1 \mathbf{u}_1 \mathbf{v}_1^T + \sigma_2 \mathbf{u}_2 \mathbf{v}_2^T<br>$$<br>where $\sigma_1, \sigma_2$ are the non-zero singular values, and $\mathbf{u}_1, \mathbf{u}_2$, $\mathbf{v}_1, \mathbf{v}_2$ are the corresponding singular vectors.</p>
<h3 id="Practical-Implications-of-Matrix-Decomposition"><a href="#Practical-Implications-of-Matrix-Decomposition" class="headerlink" title="Practical Implications of Matrix Decomposition"></a>Practical Implications of Matrix Decomposition</h3><p>Understanding matrix decomposition based on rank has profound implications in various fields:</p>
<h4 id="1-Data-Compression-and-Dimensionality-Reduction"><a href="#1-Data-Compression-and-Dimensionality-Reduction" class="headerlink" title="1. Data Compression and Dimensionality Reduction"></a>1. <strong>Data Compression and Dimensionality Reduction</strong></h4><ul>
<li><strong>Principal Component Analysis (PCA)</strong>:<ul>
<li>Utilizes SVD to identify the principal components that capture the most variance in the data.</li>
<li>Reduces the dimensionality of data by projecting it onto a lower-dimensional subspace spanned by the top singular vectors.</li>
</ul>
</li>
</ul>
<h4 id="2-Signal-Processing"><a href="#2-Signal-Processing" class="headerlink" title="2. Signal Processing"></a>2. <strong>Signal Processing</strong></h4><ul>
<li><strong>Noise Reduction</strong>:<ul>
<li>By decomposing a signal matrix and truncating smaller singular values, it’s possible to filter out noise while preserving significant signal components.</li>
</ul>
</li>
</ul>
<h4 id="3-Machine-Learning-and-Recommendation-Systems"><a href="#3-Machine-Learning-and-Recommendation-Systems" class="headerlink" title="3. Machine Learning and Recommendation Systems"></a>3. <strong>Machine Learning and Recommendation Systems</strong></h4><ul>
<li><strong>Latent Factor Models</strong>:<ul>
<li>Decompose user-item interaction matrices to uncover underlying factors that explain observed interactions, enabling personalized recommendations.</li>
</ul>
</li>
</ul>
<h4 id="4-Computer-Graphics-and-Image-Processing"><a href="#4-Computer-Graphics-and-Image-Processing" class="headerlink" title="4. Computer Graphics and Image Processing"></a>4. <strong>Computer Graphics and Image Processing</strong></h4><ul>
<li><strong>Image Compression</strong>:<ul>
<li>Represent images as low-rank matrices to reduce storage requirements without significant loss of quality.</li>
</ul>
</li>
</ul>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>Matrix decomposition based on rank is not merely an abstract mathematical exercise but a cornerstone of modern computational techniques. By decomposing matrices into sums of rank-1 components or through SVD, we gain invaluable insights into the structure and transformative capabilities of linear operators. This understanding enables us to manipulate and analyze high-dimensional data efficiently, paving the way for advancements in technology, science, and engineering.</p>
<p>Understanding how different ranks correspond to transformations into various dimensional subspaces empowers us to harness the full potential of linear algebra in practical applications. Whether compressing data, filtering signals, or making informed recommendations, matrix decomposition remains an indispensable tool in the mathematician’s and engineer’s toolkit.</p>

            </div>

            <!-- Post Comments -->
            
    <!-- 使用 DISQUS_CLICK -->
<div id="disqus-comment">
    <div id="disqus_thread"></div>

<!-- add animation -->
<style>
	.disqus_click_btn {
            line-height: 30px;
            margin: 0;
            min-width: 50px;
            padding: 0 14px;
            display: inline-block;
            font-family: "Roboto", "Helvetica", "Arial", sans-serif;
            font-size: 14px;
            font-weight: 400;
            text-transform: uppercase;
            letter-spacing: 0;
            overflow: hidden;
            will-change: box-shadow;
            transition: box-shadow .2s cubic-bezier(.4, 0, 1, 1), background-color .2s cubic-bezier(.4, 0, .2, 1), color .2s cubic-bezier(.4, 0, .2, 1);
            outline: 0;
            cursor: pointer;
            text-decoration: none;
            text-align: center;
            vertical-align: middle;
            border: 0;
            background: rgba(158, 158, 158, .2);
            box-shadow: 0 2px 2px 0 rgba(0, 0, 0, .14), 0 3px 1px -2px rgba(0, 0, 0, .2), 0 1px 5px 0 rgba(0, 0, 0, .12);
            color: #fff;
            background-color: #7EC0EE;
            text-shadow: 0
        }
</style>
	
<div class="btn_click_load" id="disqus_bt"> 
    <button class="disqus_click_btn">点击查看评论</button>
</div>

<!--
<script type="text/javascript">
$('.btn_click_load').click(function() {
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'lee'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    document.getElementById('disqus_bt').style.display = "none";
});
</script>
-->
<script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'https://li_haor.gitee.io/2024/07/14/real_meaning_of_rank/';  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'https://li_haor.gitee.io/2024/07/14/real_meaning_of_rank/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>

<script type="text/javascript">
    $('.btn_click_load').click(function() {  //click to load comments
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document;
            var s = d.createElement('script');
            s.src = '//lee.disqus.com/embed.js';
            s.setAttribute('data-timestamp', + new Date());
            (d.head || d.body).appendChild(s);
        })();
        $('.btn_click_load').css('display','none');
    });
</script>
</div>
<style>
    #disqus-comment{
        background-color: #eee;
        padding: 2pc;
    }
</style>


        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy;Powered By <a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Author  <a target="_blank" rel="noopener" href="https://github.com/haoruilee" style="border-bottom: none;">haoruilee</a></li>
				<li><a target="_blank" rel="noopener" href="https://beian.miit.gov.cn/" style="border-bottom: none;">苏ICP备2020050362号</a></li>
            </ul>
            
				<span id="busuanzi_container_site_pv"> 2025 </span> 
			
        </div>
    </div>
</body>



 	
</html>
