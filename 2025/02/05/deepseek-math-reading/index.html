<!DOCTYPE HTML>
<html>

<head>
	<link rel="bookmark"  type="image/x-icon"  href="/img/logo_miccall.webp"/>
	<link rel="shortcut icon" href="/img/logo_miccall.webp">
	
			    <title>
    Haorui Li
    </title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="/css/mic_main.css" />
    <link rel="stylesheet" href="/css/dropdownMenu.css" />
    <meta name="keywords" content="haoruilee" />
    
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css" />
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('https://i.dawnlab.me/e6a2105d63e4438f288e4ae4d97f53f2.webp') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

			    
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script async type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


    <script src="https://cdnjs.loli.net/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
    <script src="https://cdnjs.loli.net/ajax/libs/jquery.scrollex/0.2.1/jquery.scrollex.min.js"></script>
    <script src="https://cdnjs.loli.net/ajax/libs/skel/3.0.1/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
<meta name="generator" content="Hexo 7.2.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_coy.css" />
<link rel="stylesheet" href="/css/typo.css" />
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="mailto:haoruili@seu.edu.cn" class="logo">Mail Me for offical  or Chat with me for fun</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special" >
            <ul class="menu links" >
			<!-- Homepage  主页  --> 
			<li >
	            <a href="/" rel="nofollow">Home</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <!-- archives  归档   --> 
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/life/" title="CV">
		                CV
		            </a>
		        </li>
		        
		        <li>
		            <a href="/tag/" title="Contact">
		                Contact
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
                    
                    <li>
                        <a title="github" href="https://github.com/haoruilee" target="_blank" rel="noopener">
                            <i class="icon fa fa-github"></i>
                        </a>
                    </li>
                    
			</ul>
</nav>

        <div id="main" >
            <div class ="post_page_title_img" style="height: 25rem;background-image: url(https://i.dawnlab.me/bda96ce3ca698fd128bac60950b0e962.png);background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;" >
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2 >Deepseek Math Reading</h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
                <h1 id="Deepseek-Math-Paper-Reading"><a href="#Deepseek-Math-Paper-Reading" class="headerlink" title="Deepseek Math Paper Reading"></a>Deepseek Math Paper Reading</h1><h1 id="Main-Contribution"><a href="#Main-Contribution" class="headerlink" title="Main Contribution"></a>Main Contribution</h1><ul>
<li>GRPO, bring ‘group’ method to training, remove value model in workflow. “GRPO foregoes the critic model, instead estimating the baseline from group scores, significantly reducing training resources compared to Proximal Policy Optimization (PPO).”</li>
<li>A unified RL formula for RFT, DPO, PPO, GRPO</li>
<li>RL training tricks:<ul>
<li>Outcomes supervision RL is better than process superversion RL</li>
<li>Iteration RL, train reward model every several inter to make it updated </li>
</ul>
</li>
</ul>
<h1 id="From-PPO-to-GRPO"><a href="#From-PPO-to-GRPO" class="headerlink" title="From PPO to GRPO"></a>From PPO to GRPO</h1><p><a target="_blank" rel="noopener" href="https://yaih.dawn.ee/image/SPdl"><img src="https://i.dawnlab.me/b22aba461c4d16d3d5236c2ce88b63ec.png" alt="b22aba461c4d16d3d5236c2ce88b63ec.png"></a></p>
<p><a target="_blank" rel="noopener" href="https://yaih.dawn.ee/image/SJKt"><img src="https://i.dawnlab.me/6f0af8f9483755bb2f8173e94659b567.png" alt="6f0af8f9483755bb2f8173e94659b567.png"></a></p>
<p><a target="_blank" rel="noopener" href="https://yaih.dawn.ee/image/Sqw7"><img src="https://i.dawnlab.me/73dba900df24a82784afaf4c7164017f.png" alt="73dba900df24a82784afaf4c7164017f.png"></a></p>
<p><strong>Proximal Policy Optimization (PPO)</strong>  is an actor-critic RL algorithm widely used in the RL fine-tuning stage for LLMs. PPO optimizes LLMs by maximizing the following surrogate objective:</p>
<script type="math/tex; mode=display">
\mathcal{J}_{PPO}(\theta) = \mathbb{E}_{[q \sim P(Q), o \sim \pi_{\theta_{\text{old}}}(O|q)]} \frac{1}{|o|} \sum_{t=1}^{|o|} \min \left[ \frac{\pi_{\theta}(o_t|q, o<t)}{\pi_{\theta_{\text{old}}}(o_t|q, o<t)} A_t, \text{clip} \left( \frac{\pi_{\theta}(o_t|q, o<t)}{\pi_{\theta_{\text{old}}}(o_t|q, o<t)}, 1-\epsilon, 1+\epsilon \right) A_t \right]</script><p>where:</p>
<ul>
<li>$ \pi<em>{\theta} $ and $ \pi</em>{\theta_{\text{old}}} $ are the current and old policy models, respectively.</li>
<li>$ q $ and $ o $ are quesns and outputs sampled from the question dataset and the old policy $ \pi<em>{\theta</em>{\text{old}}} $.</li>
<li>$ \epsilon $ is a clipping-related hyper-parameter introduced in PPO for stabilizing training.</li>
<li>$ A<em>t $ is the <strong>advantage</strong>, computed using <strong>Generalized Advantage Estimation (GAE)</strong> (Schulman et al., 2015), based on rewards $ {r_t} $ and a learned value function $ V</em>{\psi} $.</li>
</ul>
<p>In PPO, a value function needs to be trained alongside the policy model. To mitigate over-optimization of the reward model, a <strong>per-token KL penalty</strong> from a reference model is added to the reward at each token:</p>
<script type="math/tex; mode=display">
r_t = r_{\phi}(q, o \leq t) - \beta \log \left( \frac{\pi_{\theta}(o_t|q, o<t)}{\pi_{\theta_{\text{ref}}}(o_t|q, o<t)} \right)</script><p>where:</p>
<ul>
<li>$ r_{\phi} $ is the reward model,</li>
<li>$ \pi<em>{\theta</em>{\text{ref}}} $ is the reference model (usually the initial SFT model),</li>
<li>$ \beta $ is the coefficient for the KL penalty.</li>
</ul>
<p>GRPO foregoes the value model, instead estimating the baseline from group scores, which significantly reduces training resources.</p>
<p>PPO requires a large memory and computational burden due to the value model, which is typically of comparable size to the policy model. Moreover, in LLM contexts, usually only the last token is assigned a reward score, complicating the training of an accurate value function at each token.</p>
<p>To address this, <strong>Group Relative Policy Optimization (GRPO)</strong> eliminates the need for an additional value function, using the <strong>average reward</strong> of multiple outputs produced in response to the same question as the baseline. Specifically, for each question $ q $, GRPO samples a group of outputs $ {o<em>1, o_2, \dots, o_G} $ from the old policy $ \pi</em>{\theta_{\text{old}}} $ and optimizes the policy by maximizing the following objective:</p>
<script type="math/tex; mode=display">
\mathcal{J}_{GRPO}(\theta) = \mathbb{E}_{[q \sim P(Q), \{o_i\}_i = 1^G \sim \pi_{\theta_{\text{old}}}(O|q)]} \frac{1}{G} \sum_{i=1}^{G} \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \min \left[ \frac{\pi_{\theta}(o_{i,t}|q, o_i < t)}{\pi_{\theta_{\text{old}}}(o_{i,t}|q, o_i < t)} \hat{A}^i_t, \text{clip} \left( \frac{\pi_{\theta}(o_{i,t}|q, o_i < t)}{\pi_{\theta_{\text{old}}}(o_{i,t}|q, o_i < t)}, 1-\epsilon, 1+\epsilon \right) \hat{A}^i_t \right] - \beta D_{\text{KL}}[\pi_{\theta} \| \pi_{\text{ref}}]</script><p>where:</p>
<ul>
<li>$ \hat{A}^i_t $ is the advantage computed based on the relative rewards of outputs inside each group,</li>
<li>$ \beta $ and $ \epsilon $ are hyper-parameters,</li>
<li>$ D<em>{\text{KL}}[\pi</em>{\theta} | \pi<em>{\text{ref}}] $ is the <strong>KL divergence</strong> between the trained policy $ \pi</em>{\theta} $ and the reference policy $ \pi_{\text{ref}} $.</li>
</ul>
<p>The advantage is computed relative to the group, aligning well with reward models that are typically trained on datasets comparing outputs for the same question.</p>
<h1 id="Insights-of-Reinforcement-Learn"><a href="#Insights-of-Reinforcement-Learn" class="headerlink" title="Insights of Reinforcement Learn"></a>Insights of Reinforcement Learn</h1><p>DeepseekMath also propose a unified form of RL:</p>
<p><img src="https://i.dawnlab.me/9ef2365d8a9c2bc8642b5ee0aea3b223.png" alt="9ef2365d8a9c2bc8642b5ee0aea3b223.png"></p>

            </div>

            <!-- Post Comments -->
            
    <!-- 使用 DISQUS_CLICK -->
<div id="disqus-comment">
    <div id="disqus_thread"></div>

<!-- add animation -->
<style>
	.disqus_click_btn {
            line-height: 30px;
            margin: 0;
            min-width: 50px;
            padding: 0 14px;
            display: inline-block;
            font-family: "Roboto", "Helvetica", "Arial", sans-serif;
            font-size: 14px;
            font-weight: 400;
            text-transform: uppercase;
            letter-spacing: 0;
            overflow: hidden;
            will-change: box-shadow;
            transition: box-shadow .2s cubic-bezier(.4, 0, 1, 1), background-color .2s cubic-bezier(.4, 0, .2, 1), color .2s cubic-bezier(.4, 0, .2, 1);
            outline: 0;
            cursor: pointer;
            text-decoration: none;
            text-align: center;
            vertical-align: middle;
            border: 0;
            background: rgba(158, 158, 158, .2);
            box-shadow: 0 2px 2px 0 rgba(0, 0, 0, .14), 0 3px 1px -2px rgba(0, 0, 0, .2), 0 1px 5px 0 rgba(0, 0, 0, .12);
            color: #fff;
            background-color: #7EC0EE;
            text-shadow: 0
        }
</style>
	
<div class="btn_click_load" id="disqus_bt"> 
    <button class="disqus_click_btn">点击查看评论</button>
</div>

<!--
<script type="text/javascript">
$('.btn_click_load').click(function() {
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'lee'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    document.getElementById('disqus_bt').style.display = "none";
});
</script>
-->
<script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'https://li_haor.gitee.io/2025/02/05/deepseek-math-reading/';  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'https://li_haor.gitee.io/2025/02/05/deepseek-math-reading/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>

<script type="text/javascript">
    $('.btn_click_load').click(function() {  //click to load comments
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document;
            var s = d.createElement('script');
            s.src = '//lee.disqus.com/embed.js';
            s.setAttribute('data-timestamp', + new Date());
            (d.head || d.body).appendChild(s);
        })();
        $('.btn_click_load').css('display','none');
    });
</script>
</div>
<style>
    #disqus-comment{
        background-color: #eee;
        padding: 2pc;
    }
</style>


        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy;Powered By <a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Author  <a target="_blank" rel="noopener" href="https://github.com/haoruilee" style="border-bottom: none;">haoruilee</a></li>
				<li><a target="_blank" rel="noopener" href="https://beian.miit.gov.cn/" style="border-bottom: none;">苏ICP备2020050362号</a></li>
            </ul>
            
				<span id="busuanzi_container_site_pv"> 2025 </span> 
			
        </div>
    </div>
</body>



 	
</html>
