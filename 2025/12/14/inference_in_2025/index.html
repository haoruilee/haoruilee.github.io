<!DOCTYPE HTML>
<html>

<head>
	<link rel="bookmark"  type="image/x-icon"  href="/img/logo_miccall.webp"/>
	<link rel="shortcut icon" href="/img/logo_miccall.webp">
	
			    <title>
    Haorui Li
    </title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="/css/mic_main.css" />
    <link rel="stylesheet" href="/css/dropdownMenu.css" />
    <meta name="keywords" content="haoruilee" />
    
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css" />
    </noscript>
    <!-- Preload critical images -->
    <link rel="preload" href="https://i.dawnlab.me/e6a2105d63e4438f288e4ae4d97f53f2.webp" as="image">
    <link rel="preload" href="/images/pusheencode.gif" as="image">

    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('https://i.dawnlab.me/e6a2105d63e4438f288e4ae4d97f53f2.webp') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover;
        }
    </style>

			    
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script async type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


    <script src="https://cdnjs.loli.net/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
    <script src="https://cdnjs.loli.net/ajax/libs/jquery.scrollex/0.2.1/jquery.scrollex.min.js"></script>
    <script src="https://cdnjs.loli.net/ajax/libs/skel/3.0.1/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
<meta name="generator" content="Hexo 7.2.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_coy.css" />
<link rel="stylesheet" href="/css/typo.css" />
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="mailto:haoruili@seu.edu.cn" class="logo">Mail Me for offical  or Chat with me for fun</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special" >
            <ul class="menu links" >
			<!-- Homepage  主页  --> 
			<li >
	            <a href="/" rel="nofollow">Home</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <!-- archives  归档   --> 
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/life/" title="CV">
		                CV
		            </a>
		        </li>
		        
		        <li>
		            <a href="/tag/" title="Contact">
		                Contact
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
                    
                    <li>
                        <a title="github" href="https://github.com/haoruilee" target="_blank" rel="noopener">
                            <i class="icon fa fa-github"></i>
                        </a>
                    </li>
                    
			</ul>
</nav>

        <div id="main" >
            <div class ="post_page_title_img lazy-bg" data-bg="https://i.dawnlab.me/ad1bfef8196cd7dbc0139271eb7515a7.png" style="height: 25rem;background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;" >
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2 >How we inference (2025)</h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
                <h1 id="2025-Inference-Optimization-Notes"><a href="#2025-Inference-Optimization-Notes" class="headerlink" title="2025 Inference Optimization Notes"></a>2025 Inference Optimization Notes</h1><p>After a year working at an inference engine start up, I have witnessed the great evolution of inference optimization in 2025.</p>
<p>This blog concludes several famous breakthroughs.</p>
<hr>
<h2 id="1-The-baseline"><a href="#1-The-baseline" class="headerlink" title="1) The baseline"></a>1) The baseline</h2><p>Transformer attention (per layer) is:</p>
<p>$$<br>\mathrm{Attn}(Q,K,V)&#x3D;\mathrm{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V<br>$$</p>
<ul>
<li><strong>Prefill</strong>: tends to be heavy on compute and scales poorly with long context, often the attention term dominates.</li>
<li><strong>Decode</strong>: becomes a mix of <em>serial dependency</em> + <em>KV cache bandwidth&#x2F;latency</em>, especially once you are serving at scale.</li>
</ul>
<p>Nearly every big inference win in 2025 is either:</p>
<ol>
<li>making KV cheaper (smaller, more compact, more reusable), or</li>
<li>separating and scheduling phases to reduce interference and queueing.</li>
</ol>
<hr>
<h2 id="2-KV-cache"><a href="#2-KV-cache" class="headerlink" title="2) KV cache"></a>2) KV cache</h2><p>If you serve real traffic, the KV cache is huge, dynamic, and fragmentation-prone. Treating KV like a single contiguous blob forces you into bad trade-offs: low batch size, frequent compaction&#x2F;copies, and wasted memory.</p>
<h3 id="PagedAttention"><a href="#PagedAttention" class="headerlink" title="PagedAttention"></a>PagedAttention</h3><p>The canonical “this is how you do it” reference is <strong>PagedAttention</strong> and the system built around it, <strong>vLLM</strong>. It uses paging-style memory management to make KV allocation flexible and near-fragmentation-free, enabling larger batches and better throughput. vLLM reports <strong>~2–4× throughput improvement</strong> (same latency level) versus prior baselines, with larger gains for longer sequences and more complex decoding.</p>
<p><img src="https://i.dawnlab.me/34cd47caf474b11ef2d9057fbdfb122f.png" alt="34cd47caf474b11ef2d9057fbdfb122f.png"></p>
<hr>
<h2 id="3-PD-disaggregation"><a href="#3-PD-disaggregation" class="headerlink" title="3) PD disaggregation"></a>3) PD disaggregation</h2><p>If you colocate prefill and decode in the same pool, you get two common pathologies:</p>
<ul>
<li>prefill bursts interfere with decode latency (TPOT spikes),</li>
<li>decode pressure forces conservative batching that underutilizes prefill.</li>
</ul>
<h3 id="PD-disaggregation"><a href="#PD-disaggregation" class="headerlink" title="PD disaggregation"></a>PD disaggregation</h3><p><strong>DistServe</strong> explicitly proposes <strong>disaggregating prefill and decoding onto different GPUs</strong> to remove interference, then co-optimizing both sides for TTFT (time-to-first-token) and TPOT (time-per-output-token). It frames success as <strong>goodput under SLO constraints</strong> (not just raw tokens&#x2F;sec).</p>
<p>A simple queueing-style decomposition you can include:</p>
<p>$$<br>T \approx (W_p + S_p) + (W_d + S_d)<br>$$</p>
<p>where (S) is service time and (W) is queueing delay for prefill (p) and decode (d). PD disaggregation is largely about driving down the queueing&#x2F;interference terms (W_p, W_d) under mixed workloads.</p>
<h3 id="Making-PD-disaggregation-less-brittle"><a href="#Making-PD-disaggregation-less-brittle" class="headerlink" title="Making PD disaggregation less brittle"></a>Making PD disaggregation less brittle</h3><ul>
<li><strong>WindServe</strong> pushes phase-disaggregated serving further with <strong>stream-based, fine-grained dynamic scheduling</strong>, aiming to improve utilization and reduce stalls across the split pipeline.</li>
<li><strong>DOPD</strong> targets a very real failure mode: <strong>producer–consumer imbalance</strong> between prefill instances and decode instances. It dynamically adjusts the P&#x2F;D ratio based on load monitoring to improve goodput and tail latency.</li>
</ul>
<hr>
<h2 id="4-Large-EP-for-MoE"><a href="#4-Large-EP-for-MoE" class="headerlink" title="4) Large EP for MoE"></a>4) Large EP for MoE</h2><p>MoE inference often looks like:</p>
<p>$$<br>y&#x3D;\sum_{e \in \mathrm{TopK}(g(x))} p_e , f_e(x)<br>$$</p>
<p>At scale, the pain isn’t the math inside experts; it’s <strong>routing activations to experts across devices</strong>, typically via <strong>all-to-all</strong> collectives. That’s why “bigger EP” can make things worse unless you tame communication.</p>
<h3 id="Speculative-MoE"><a href="#Speculative-MoE" class="headerlink" title="Speculative MoE"></a>Speculative MoE</h3><p>A concrete 2025 paper here is <strong>Speculative MoE</strong>, which analyzes DeepSpeed-MoE’s EP bottleneck and proposes speculative token&#x2F;expert pre-scheduling to <strong>losslessly trim EP all-to-all communication volume</strong>, implemented in both DeepSpeed-MoE and SGLang.</p>
<p><img src="https://i.dawnlab.me/896b93dcaebc246a042113e0bc714164.png" alt="896b93dcaebc246a042113e0bc714164.png"></p>
<hr>
<h2 id="5-MLA"><a href="#5-MLA" class="headerlink" title="5) MLA"></a>5) MLA</h2><p>If decode is KV-bandwidth-bound, then reducing KV size is one of the cleanest wins.</p>
<p><strong>DeepSeek-V2</strong> introduces <strong>Multi-head Latent Attention (MLA)</strong> as an architectural change for more efficient inference (alongside its MoE design). The key pitch: MLA reduces the KV-cache footprint by projecting into a compact latent space, easing bandwidth pressure in decode.</p>
<p><img src="https://i.dawnlab.me/0b3e8ab7c9f248e508895344fe47f181.png" alt="0b3e8ab7c9f248e508895344fe47f181.png"></p>
<hr>
<h2 id="6-FP8-FP4"><a href="#6-FP8-FP4" class="headerlink" title="6) FP8, FP4"></a>6) FP8, FP4</h2><p>FP8 is the “default low-precision workhorse” in many stacks, but 2025 is where <strong>FP4 becomes a first-class inference precision</strong> (with stricter conditions).</p>
<h3 id="FP8-formats-E4M3-and-E5M2"><a href="#FP8-formats-E4M3-and-E5M2" class="headerlink" title="FP8 formats: E4M3 and E5M2"></a>FP8 formats: E4M3 and E5M2</h3><p>The standard citation is <strong>FP8 Formats for Deep Learning</strong>, proposing FP8 encodings <strong>E4M3</strong> and <strong>E5M2</strong> and showing they can retain quality close to 16-bit baselines across tasks.</p>
<p>A minimal quantization-style formula you can include (conceptual, not claiming implementation detail):</p>
<p>$$<br>x_q&#x3D;\mathrm{round}(x&#x2F;s),\quad \hat{x}&#x3D;s\cdot x_q<br>$$</p>
<h3 id="NVIDIA’s-hardware-support"><a href="#NVIDIA’s-hardware-support" class="headerlink" title="NVIDIA’s hardware support:"></a>NVIDIA’s hardware support:</h3><ul>
<li>H100 introduces FP8 for higher GEMM throughput,</li>
<li>Blackwell adds <strong>NVFP4</strong> and <strong>MXFP8</strong> support.</li>
</ul>
<hr>
<h2 id="7-MTP-Multi-Token-Prediction-as-an-inference-enabler"><a href="#7-MTP-Multi-Token-Prediction-as-an-inference-enabler" class="headerlink" title="7) MTP: Multi-Token Prediction as an inference enabler"></a>7) MTP: Multi-Token Prediction as an inference enabler</h2><p>MTP matters here because it encourages the model to predict more than one future token per step during training, which can be used to support faster decoding strategies in some setups.</p>
<h3 id="DeepSeek-V3-MTP-called-out-explicitly"><a href="#DeepSeek-V3-MTP-called-out-explicitly" class="headerlink" title="DeepSeek-V3: MTP called out explicitly"></a>DeepSeek-V3: MTP called out explicitly</h3><p>The <strong>DeepSeek-V3 Technical Report</strong> and its official repo explicitly mention a <strong>Multi-Token Prediction (MTP) objective</strong> and note it can be used for inference acceleration.</p>
<p>A generic MTP-style loss term:</p>
<p>$$<br>\mathcal{L}<em>{\text{MTP}}&#x3D;\sum_t\sum</em>{j&#x3D;1}^{m}\lambda_j,\mathrm{CE}\left(p_{\theta,j}(x_{t+j}\mid x_{\le t}),x_{t+j}\right)<br>$$</p>
<p><img src="https://i.dawnlab.me/e943ec0bc3c97fa1cf0a5ee81cc3f45c.png" alt="e943ec0bc3c97fa1cf0a5ee81cc3f45c.png"></p>
<hr>
<h2 id="8-DSA-sparse-attention-to-break-the-long-context-prefill-wall"><a href="#8-DSA-sparse-attention-to-break-the-long-context-prefill-wall" class="headerlink" title="8) DSA: sparse attention to break the long-context prefill wall"></a>8) DSA: sparse attention to break the long-context prefill wall</h2><p>Long context makes dense attention expensive. Sparse attention replaces “attend to everything” with “attend to a selected subset”.</p>
<p>A simple way to write it:</p>
<p>$$<br>a_t&#x3D;\mathrm{softmax}\left(\frac{q_t K_{S_t}^\top}{\sqrt{d_k}}\right)V_{S_t},\quad |S_t|&#x3D;k\ll L<br>$$</p>
<p>This reduces cost from roughly (O(L^2)) to (O(Lk)) (depending on indexing and sparsity structure).</p>
<h3 id="DeepSeek-V3-2-DeepSeek-Sparse-Attention-DSA"><a href="#DeepSeek-V3-2-DeepSeek-Sparse-Attention-DSA" class="headerlink" title="DeepSeek-V3.2: DeepSeek Sparse Attention (DSA)"></a>DeepSeek-V3.2: DeepSeek Sparse Attention (DSA)</h3><p><strong>DeepSeek-V3.2</strong> explicitly presents <strong>DSA</strong> as a key breakthrough for long-context efficiency, stating it substantially reduces computational complexity while preserving performance.</p>
<h3 id="FlashMLA-kernels-and-the-“real”-implementation-surface"><a href="#FlashMLA-kernels-and-the-“real”-implementation-surface" class="headerlink" title="FlashMLA: kernels and the “real” implementation surface"></a>FlashMLA: kernels and the “real” implementation surface</h3><p>DeepSeek’s <strong>FlashMLA</strong> repository states that it releases token-level sparse attention kernels used for DSA and provides a deep-dive doc on FP8 sparse decoding &#x2F; FP8 KV cache format.<br>If you want a serving-engine-facing reference, vLLM’s <code>flashmla</code> API docs explicitly mention FP8 KV cache and sparse indices enabling sparse attention.</p>
<p><a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/FlashMLA">https://github.com/deepseek-ai/FlashMLA</a></p>
<hr>
<h2 id="9-Putting-it-together"><a href="#9-Putting-it-together" class="headerlink" title="9) Putting it together"></a>9) Putting it together</h2><p>According to Amdahl’s law:</p>
<p>$$<br>\text{speedup}&#x3D;\frac{1}{\sum_i \frac{f_i}{s_i}}<br>$$</p>
<p>Each optimization only accelerates the fraction (f_i) of time you actually spend in that component. The real reason these ideas compound is that they move the bottleneck:</p>
<ul>
<li>KV paging increases achievable batch size (vLLM&#x2F;PagedAttention).</li>
<li>PD disaggregation increases goodput under TTFT&#x2F;TPOT SLOs (DistServe&#x2F;WindServe&#x2F;DOPD).</li>
<li>MLA reduces KV footprint structurally (DeepSeek-V2).</li>
<li>FP8&#x2F;FP4 reduce compute+bandwidth per op (FP8 formats + TE + NVFP4 + MX spec).</li>
<li>EP optimizations reduce all-to-all pain (Speculative MoE).</li>
<li>DSA makes long-context prefill feasible at lower cost (DeepSeek-V3.2 + FlashMLA).</li>
</ul>
<hr>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><h2 id="KV-cache-memory-management"><a href="#KV-cache-memory-management" class="headerlink" title="KV cache &#x2F; memory management"></a>KV cache &#x2F; memory management</h2><ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2309.06180">PagedAttention</a>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2309.06180">https://arxiv.org/abs/2309.06180</a></li>
</ul>
<h2 id="PD-disaggregation-1"><a href="#PD-disaggregation-1" class="headerlink" title="PD disaggregation"></a>PD disaggregation</h2><ul>
<li>DistServe:<a target="_blank" rel="noopener" href="https://www.usenix.org/system/files/osdi24-zhong-yinmin.pdf">https://www.usenix.org/system/files/osdi24-zhong-yinmin.pdf</a></li>
<li>WindServe: <a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3695053.3730999">https://dl.acm.org/doi/10.1145/3695053.3730999</a></li>
<li>DOPD:<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.20982">https://arxiv.org/abs/2511.20982</a></li>
</ul>
<h2 id="MoE-EP"><a href="#MoE-EP" class="headerlink" title="MoE EP"></a>MoE EP</h2><ul>
<li>Speculative MoE:<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2503.04398">https://arxiv.org/abs/2503.04398</a></li>
</ul>
<h2 id="MLA-MTP-DSA"><a href="#MLA-MTP-DSA" class="headerlink" title="MLA &#x2F; MTP &#x2F; DSA"></a>MLA &#x2F; MTP &#x2F; DSA</h2><ul>
<li>DeepSeek-V2 (MLA):<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.04434">https://arxiv.org/abs/2405.04434</a></li>
<li>DeepSeek-V3.2 (DSA) arXiv:<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.02556">https://arxiv.org/abs/2512.02556</a></li>
<li>FlashMLA repo (kernels + FP8 sparse deep dive doc):<br><a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/FlashMLA">https://github.com/deepseek-ai/FlashMLA</a></li>
</ul>
<h2 id="FP8-FP4-MX"><a href="#FP8-FP4-MX" class="headerlink" title="FP8 &#x2F; FP4 &#x2F; MX"></a>FP8 &#x2F; FP4 &#x2F; MX</h2><ul>
<li>FP8 Formats for Deep Learning:<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2209.05433">https://arxiv.org/abs/2209.05433</a></li>
<li>NVIDIA Transformer Engine “Using FP8 and FP4” doc:<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html">https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html</a></li>
<li>NVIDIA blog: Introducing NVFP4 (Blackwell):<a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/introducing-nvfp4-for-efficient-and-accurate-low-precision-inference/">https://developer.nvidia.com/blog/introducing-nvfp4-for-efficient-and-accurate-low-precision-inference/</a></li>
<li>OCP MX Spec v1.0:<br><a target="_blank" rel="noopener" href="https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf">https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf</a></li>
</ul>

            </div>

            <!-- Post Comments -->
            
    <!-- 使用 DISQUS_CLICK -->
<div id="disqus-comment">
    <div id="disqus_thread"></div>

<!-- add animation -->
<style>
	.disqus_click_btn {
            line-height: 30px;
            margin: 0;
            min-width: 50px;
            padding: 0 14px;
            display: inline-block;
            font-family: "Roboto", "Helvetica", "Arial", sans-serif;
            font-size: 14px;
            font-weight: 400;
            text-transform: uppercase;
            letter-spacing: 0;
            overflow: hidden;
            will-change: box-shadow;
            transition: box-shadow .2s cubic-bezier(.4, 0, 1, 1), background-color .2s cubic-bezier(.4, 0, .2, 1), color .2s cubic-bezier(.4, 0, .2, 1);
            outline: 0;
            cursor: pointer;
            text-decoration: none;
            text-align: center;
            vertical-align: middle;
            border: 0;
            background: rgba(158, 158, 158, .2);
            box-shadow: 0 2px 2px 0 rgba(0, 0, 0, .14), 0 3px 1px -2px rgba(0, 0, 0, .2), 0 1px 5px 0 rgba(0, 0, 0, .12);
            color: #fff;
            background-color: #7EC0EE;
            text-shadow: 0
        }
</style>
	
<div class="btn_click_load" id="disqus_bt"> 
    <button class="disqus_click_btn">点击查看评论</button>
</div>

<!--
<script type="text/javascript">
$('.btn_click_load').click(function() {
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'lee'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    document.getElementById('disqus_bt').style.display = "none";
});
</script>
-->
<script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'https://li_haor.gitee.io/2025/12/14/inference_in_2025/';  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'https://li_haor.gitee.io/2025/12/14/inference_in_2025/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>

<script type="text/javascript">
    $('.btn_click_load').click(function() {  //click to load comments
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document;
            var s = d.createElement('script');
            s.src = '//lee.disqus.com/embed.js';
            s.setAttribute('data-timestamp', + new Date());
            (d.head || d.body).appendChild(s);
        })();
        $('.btn_click_load').css('display','none');
    });
</script>
</div>
<style>
    #disqus-comment{
        background-color: #eee;
        padding: 2pc;
    }
</style>


        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy;Powered By <a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Author  <a target="_blank" rel="noopener" href="https://github.com/haoruilee" style="border-bottom: none;">haoruilee</a></li>
				<li><a target="_blank" rel="noopener" href="https://beian.miit.gov.cn/" style="border-bottom: none;">苏ICP备2020050362号</a></li>
            </ul>
            
				<span id="busuanzi_container_site_pv"> 2025 </span> 
			
        </div>
    </div>
</body>



 	
</html>
