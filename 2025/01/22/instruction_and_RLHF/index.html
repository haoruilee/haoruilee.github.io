<!DOCTYPE HTML>
<html>

<head>
	<link rel="bookmark"  type="image/x-icon"  href="/img/logo_miccall.webp"/>
	<link rel="shortcut icon" href="/img/logo_miccall.webp">
	
			    <title>
    Haorui Li
    </title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="/css/mic_main.css" />
    <link rel="stylesheet" href="/css/dropdownMenu.css" />
    <meta name="keywords" content="haoruilee" />
    
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css" />
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('https://i.dawnlab.me/e6a2105d63e4438f288e4ae4d97f53f2.webp') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

			    
  


    <script src="https://cdnjs.loli.net/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
    <script src="https://cdnjs.loli.net/ajax/libs/jquery.scrollex/0.2.1/jquery.scrollex.min.js"></script>
    <script src="https://cdnjs.loli.net/ajax/libs/skel/3.0.1/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
<meta name="generator" content="Hexo 7.2.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_coy.css" />
<link rel="stylesheet" href="/css/typo.css" />
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="mailto:haoruili@seu.edu.cn" class="logo">Mail Me for offical  or Chat with me for fun</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special" >
            <ul class="menu links" >
			<!-- Homepage  主页  --> 
			<li >
	            <a href="/" rel="nofollow">Home</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <!-- archives  归档   --> 
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/life/" title="CV">
		                CV
		            </a>
		        </li>
		        
		        <li>
		            <a href="/tag/" title="Contact">
		                Contact
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
                    
                    <li>
                        <a title="github" href="https://github.com/haoruilee" target="_blank" rel="noopener">
                            <i class="icon fa fa-github"></i>
                        </a>
                    </li>
                    
			</ul>
</nav>

        <div id="main" >
            <div class ="post_page_title_img" style="height: 25rem;background-image: url(https://i.dawnlab.me/2fe1fc912793db3afd36a8823daca1c5.png);background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;" >
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2 >Everything about LLM Align</h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
                <h1 id="Everything-about-LLM-Align"><a href="#Everything-about-LLM-Align" class="headerlink" title="Everything about LLM Align"></a>Everything about LLM Align</h1><p>Source:</p>
<ul>
<li>Aligning language models to follow instructions <a target="_blank" rel="noopener" href="https://openai.com/index/instruction-following/">https://openai.com/index/instruction-following/</a></li>
<li>A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO, DPO and More  <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.16216">https://arxiv.org/pdf/2407.16216</a></li>
<li>Learning from human preferences <a target="_blank" rel="noopener" href="https://openai.com/index/learning-from-human-preferences/">https://openai.com/index/learning-from-human-preferences/</a> </li>
<li>Deep reinforcement learning from human preferences <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03741">https://arxiv.org/abs/1706.03741</a></li>
<li>Detailed Formulas <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/7461863937">https://zhuanlan.zhihu.com/p/7461863937</a></li>
<li>DeepSpeed Chat <a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/README.md">https://github.com/microsoft/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/README.md</a></li>
</ul>
<p>Note: This blog is a small part of these sources, recommend read them all.</p>
<h1 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL; DR"></a>TL; DR</h1><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2407.16216">This paper</a> categorizes techniques for aligning large language models (LLMs) into <strong>four main themes</strong> with subtopics:</p>
<ol>
<li><p><strong>Reward Models</strong>  </p>
<ul>
<li>Explicit vs. Implicit Models  </li>
<li>Pointwise vs. Pairwise Preference Models  </li>
<li>Response-Level vs. Token-Level Rewards  </li>
<li>Negative Preference Optimization</li>
</ul>
</li>
<li><p><strong>Feedback</strong>  </p>
<ul>
<li>Preference Feedback vs. Binary Feedback  </li>
<li>Pairwise vs. Listwise Feedback  </li>
<li>Human Feedback vs. AI Feedback</li>
</ul>
</li>
<li><p><strong>Reinforcement Learning (RL)</strong>  </p>
<ul>
<li>Reference-Based RL vs. Reference-Free RL  </li>
<li>Length-Controlled RL  </li>
<li>Online vs. Offline RL Strategies  </li>
<li>Subfields of RL Techniques</li>
</ul>
</li>
<li><p><strong>Optimization</strong>  </p>
<ul>
<li>Iterative (Online) vs. Non-Iterative (Offline) Preference Optimization  </li>
<li>Separate vs. Combined SFT (Supervised Fine-Tuning) and Alignment</li>
</ul>
</li>
</ol>
<h3 id="Key-Techniques"><a href="#Key-Techniques" class="headerlink" title="Key Techniques:"></a>Key Techniques:</h3><ul>
<li><strong>RLHF (Reinforcement Learning from Human Feedback):</strong> Aligns LLMs with user intent using human-labeled rewards and policies.  </li>
<li><strong>RLAIF (Reinforcement Learning from AI Feedback):</strong> Utilizes AI-generated feedback to enhance alignment.  </li>
<li><strong>Direct Preference Optimization (DPO):</strong> Aligns LLMs by directly optimizing preferences rather than modeling reward signals.  </li>
<li><strong>Iterative&#x2F;Online Methods:</strong> Enables dynamic fine-tuning for out-of-distribution tasks.</li>
</ul>
<h3 id="Emerging-Trends"><a href="#Emerging-Trends" class="headerlink" title="Emerging Trends:"></a>Emerging Trends:</h3><ul>
<li><strong>Token-Level Optimization</strong> for granular alignment.  </li>
<li><strong>Binary Feedback Systems</strong> for scalability.  </li>
<li><strong>Nash Learning Approaches</strong> to address inconsistencies in pairwise preferences.  </li>
<li><strong>Combined SFT and Alignment</strong> to improve efficiency while mitigating catastrophic forgetting.</li>
</ul>
<h1 id="Let’s-start-from-RL"><a href="#Let’s-start-from-RL" class="headerlink" title="Let’s start from RL"></a>Let’s start from RL</h1><p><em>Skipped all inference here</em></p>
<p><strong>TL; DR</strong>: In RL, the Actor-Critic framework combines policy-based and value-based methods, where the actor updates the policy by choosing actions, and the critic evaluates the actions by estimating the value function, helping to reduce variance and improve learning efficiency.</p>
<h3 id="Key-Concepts-and-Notations"><a href="#Key-Concepts-and-Notations" class="headerlink" title="Key Concepts and Notations"></a>Key Concepts and Notations</h3><ul>
<li><p><strong>State ($s_t$)</strong>: The environment’s state at time step t.</p>
</li>
<li><p><strong>Action ($a_t$)</strong>: The action taken by the agent at time step t.</p>
</li>
<li><p><strong>Policy ($\pi_\theta(a_t | s_t)$)</strong>: A parameterized probability distribution over actions a_t given state s_t.</p>
</li>
<li><p><strong>Reward ($r_t$)</strong>: The scalar feedback received after taking action a_t in state s_t.</p>
</li>
<li><p><strong>Return ($V_t$)</strong>: The cumulative reward starting from time step t:</p>
<p>$$<br>V_t &#x3D; \sum_{k&#x3D;0}^\infty \gamma^k r_{t+k+1}<br>$$</p>
<p>where $\gamma \in [0, 1)$ is the discount factor.</p>
</li>
</ul>
<hr>
<h3 id="Objective-Function"><a href="#Objective-Function" class="headerlink" title="Objective Function"></a>Objective Function</h3><p>The goal is to <strong>maximize the expected cumulative reward</strong>:</p>
<p>$$<br>J(\theta) &#x3D; \mathbb{E}<em>{\pi_\theta} \left[ V_t \right] &#x3D; \mathbb{E}</em>{\pi_\theta} \left[ \sum_{t&#x3D;0}^\infty r_t \right]<br>$$</p>
<p>Using the <strong>policy gradient theorem</strong>, the gradient of $J(\theta)$ is:</p>
<p>$$<br>\nabla_\theta J(\theta) &#x3D; \mathbb{E}<em>{\tau \sim \pi_\theta} \left[ \sum</em>{t&#x3D;0}^\infty \nabla_\theta \log \pi_\theta(a_t | s_t) V_t \right]<br>$$</p>
<hr>
<h3 id="Advantage-Function"><a href="#Advantage-Function" class="headerlink" title="Advantage Function"></a>Advantage Function</h3><p>To reduce variance, the <strong>advantage function</strong> $A_t$ is often used instead of $V_t$:</p>
<p>$$<br>A_t &#x3D; Q^\pi(s_t, a_t) - V^\pi(s_t)<br>$$</p>
<p>Definitions:</p>
<ul>
<li><strong>State-value function ($V^\pi(s_t)$)</strong>:</li>
</ul>
<p>$$<br>V^\pi(s_t) &#x3D; \mathbb{E}_{a_t \sim \pi_\theta} \left[ V_t \mid s_t \right]<br>$$</p>
<ul>
<li><strong>Action-value function ($Q^\pi(s_t, a_t)$)</strong>:</li>
</ul>
<p>$$<br>Q^\pi(s_t, a_t) &#x3D; \mathbb{E}_{\tau \sim \pi_\theta} \left[ V_t \mid s_t, a_t \right]<br>$$</p>
<p>Using the advantage function, the gradient becomes:</p>
<p>$$<br>\nabla_\theta J(\theta) &#x3D; \mathbb{E}<em>{\tau \sim \pi_\theta} \left[ \sum</em>{t&#x3D;0}^\infty \nabla_\theta \log \pi_\theta(a_t | s_t) A^\pi(s_t, a_t) \right]<br>$$</p>
<hr>
<h3 id="Actor-Critic-Framework"><a href="#Actor-Critic-Framework" class="headerlink" title="Actor-Critic Framework"></a>Actor-Critic Framework</h3><p>The <strong>Actor-Critic</strong> method combines policy-based and value-based approaches:</p>
<ul>
<li><strong>Actor</strong>: Represents the policy $\pi_\theta(a_t | s_t)$.</li>
<li><strong>Critic</strong>: Estimates the value function $V^\pi(s_t)$ or $Q^\pi(s_t, a_t)$.</li>
</ul>
<p>The critic minimizes the <strong>Temporal Difference (TD) error</strong>:</p>
<p>$$<br>\delta_t &#x3D; r_t + \gamma V^\pi(s_{t+1}) - V^\pi(s_t)<br>$$</p>
<p>The actor updates the policy using:</p>
<p>$$<br>\nabla_\theta J(\theta) &#x3D; \mathbb{E}_{\tau \sim \pi_\theta} \left[ \nabla_\theta \log \pi_\theta(a_t | s_t) \delta_t \right]<br>$$</p>
<hr>
<h3 id="Generalized-Advantage-Estimation-GAE"><a href="#Generalized-Advantage-Estimation-GAE" class="headerlink" title="Generalized Advantage Estimation (GAE)"></a>Generalized Advantage Estimation (GAE)</h3><p>The <strong>Generalized Advantage Estimation (GAE)</strong> smooths the advantage function for better bias-variance trade-off:</p>
<p>$$<br>A_t^\text{GAE} &#x3D; \sum_{k&#x3D;0}^\infty (\gamma \lambda)^k \delta_{t+k}<br>$$</p>
<p>where:</p>
<ul>
<li>$\lambda \in [0, 1]$: A smoothing parameter.</li>
<li>$\delta_t &#x3D; r_t + \gamma V^\pi(s_{t+1}) - V^\pi(s_t)$.</li>
</ul>
<hr>
<h3 id="Algorithm-Summary-Vanilla-Policy-Gradient"><a href="#Algorithm-Summary-Vanilla-Policy-Gradient" class="headerlink" title="Algorithm Summary (Vanilla Policy Gradient)"></a>Algorithm Summary (Vanilla Policy Gradient)</h3><ol>
<li><strong>Sample Trajectories</strong>: Collect N trajectories ${\tau_i}_{i&#x3D;1}^N$ using the current policy $\pi_\theta$.</li>
<li><strong>Estimate Rewards</strong>: Compute $V_t$ or $A_t$ for each time step.</li>
<li><strong>Compute Policy Gradient</strong>:</li>
</ol>
<p>$$<br>\nabla_\theta J(\theta) &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^N \sum_{t&#x3D;0}^{T_i} \nabla_\theta \log \pi_\theta(a_t^i | s_t^i) A_t^i<br>$$</p>
<ol start="4">
<li><strong>Update Policy</strong>:</li>
</ol>
<p>$$<br>\theta \gets \theta + \alpha \nabla_\theta J(\theta)<br>$$</p>
<p>where $\alpha$ is the learning rate.</p>
<h3 id="Comparison-Policy-based-vs-Value-based-RL"><a href="#Comparison-Policy-based-vs-Value-based-RL" class="headerlink" title="Comparison: Policy-based vs. Value-based RL"></a>Comparison: Policy-based vs. Value-based RL</h3><table>
<thead>
<tr>
<th><strong>Aspect</strong></th>
<th><strong>Policy-based RL</strong></th>
<th><strong>Value-based RL</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>Core Objective</strong></td>
<td>Directly learns the policy $\pi_\theta(a</td>
<td>s)$.</td>
</tr>
<tr>
<td><strong>Strategy</strong></td>
<td>Optimizes the policy to maximize cumulative rewards.</td>
<td>Derives the policy indirectly by maximizing the value function.</td>
</tr>
<tr>
<td><strong>Action Space</strong></td>
<td>Handles both continuous and discrete action spaces well.</td>
<td>Primarily suited for discrete action spaces.</td>
</tr>
<tr>
<td><strong>Exploration</strong></td>
<td>Naturally supports stochastic exploration.</td>
<td>Requires $\epsilon$-greedy or similar exploration strategies.</td>
</tr>
<tr>
<td><strong>Convergence</strong></td>
<td>More stable due to directly optimizing the objective.</td>
<td>Can oscillate as policy is derived from value updates.</td>
</tr>
<tr>
<td><strong>Sample Efficiency</strong></td>
<td>Less efficient; requires many samples for policy updates.</td>
<td>More sample efficient due to value function updates.</td>
</tr>
<tr>
<td><strong>Common Methods</strong></td>
<td>REINFORCE, PPO, TRPO, Actor-Critic.</td>
<td>Q-Learning, DQN, SARSA.</td>
</tr>
</tbody></table>
<h1 id="From-Actor-Critic-to-LLM-RLHF"><a href="#From-Actor-Critic-to-LLM-RLHF" class="headerlink" title="From Actor-Critic to LLM RLHF"></a>From Actor-Critic to LLM RLHF</h1><p><img src="https://i.dawnlab.me/9f3e63e25cda387daeea097e6b0d8228.png" alt="9f3e63e25cda387daeea097e6b0d8228.png"></p>
<p>So, how do we map this RL process to NLP tasks? In other words, what do the agent, environment, state, action, etc., represent in the context of NLP tasks?</p>
<p>Recall the purpose of applying reinforcement learning (RLHF) to NLP tasks: we want to provide a model with a prompt, and the model should generate responses that align with human preferences. Now, recall the reasoning process of the GPT model: at each step, the model generates a single token, meaning tokens are produced one after another, with the current token relying on the previous token.</p>
<p>With these points in mind, we can now better interpret the diagram above:</p>
<ol>
<li>We first provide the model with a prompt and expect it to generate a response that aligns with human preferences.</li>
<li>At time step $t_1$, the model generates a token $a_1$ based on the prompt and previous context, and this token corresponds to the action in reinforcement learning. Hence, it’s easy to understand that, in the NLP context, the action space of the RL task corresponds to the vocabulary.</li>
<li>At time step $t_2$, the model generates token $a_2$, and the immediate reward $r_2$ is obtained, with the total return $V_2$ being the sum of all future rewards (recall that $V_t$ encapsulates both “immediate rewards” and “future rewards”).</li>
<li>This reward can be understood as a “measure of human preference.” At this point, the model’s state transitions from $s_1$ to $s_2$, i.e., from the “previous context” to the “previous context + new token.”</li>
<li>In the NLP context, the agent is the language model itself, and the environment corresponds to the text it generates.</li>
</ol>
<p>Note: $R_t$ (the immediate reward) and $V_t$ (the total return) are not directly generated by the language model. They are instead generated by two other models, which will be explained in detail later.</p>
<h1 id="RLHF-PPO"><a href="#RLHF-PPO" class="headerlink" title="RLHF-PPO"></a>RLHF-PPO</h1><p><img src="https://i.dawnlab.me/40f05f198353b1b85b67bbd515f8dad1.png" alt="40f05f198353b1b85b67bbd515f8dad1.png"></p>
<p>As shown in the diagram, during the RLHF-PPO stage, there are four main models:</p>
<ul>
<li><strong>Actor Model</strong>: The actor model is the target language model we want to train.</li>
<li><strong>Critic Model</strong>: The critic model predicts the total return $V_t$.</li>
<li><strong>Reward Model</strong>: The reward model calculates the immediate reward $r_t$.</li>
<li><strong>Reference Model</strong>: The reference model is used to impose constraints on the language model during the RLHF phase to prevent it from deviating too much (i.e., it prevents the model from updating in an uncontrolled manner, which could lead to worse performance).</li>
</ul>
<h3 id="Details-of-the-Models"><a href="#Details-of-the-Models" class="headerlink" title="Details of the Models"></a>Details of the Models</h3><ul>
<li><strong>Actor&#x2F;Critic Models</strong> need to be trained during the RLHF phase (these models are highlighted with thick borders in the diagram). On the other hand, the <strong>Reward&#x2F;Reference Models</strong> have their parameters frozen.</li>
<li>The <strong>Critic&#x2F;Reward&#x2F;Reference Models</strong> collectively form a “reward-loss” calculation system (a term I coined for better understanding), where we compute the loss by combining their results to update the Actor and Critic models.</li>
</ul>
<p>Now, let’s break down each of these models in more detail:</p>
<h3 id="Actor-Model"><a href="#Actor-Model" class="headerlink" title="Actor Model"></a>Actor Model</h3><p>As mentioned earlier, the <strong>Actor</strong> is the language model we aim to train. Typically, we initialize it using the model generated from the <strong>Supervised Fine-Tuning (SFT)</strong> stage.</p>
<p><img src="https://i.dawnlab.me/9edb5299218035822b31597c4ceb5b49.png" alt="9edb5299218035822b31597c4ceb5b49.png"></p>
<p>The goal is to train the Actor model to generate responses that align with human preferences. Our strategy is to first provide the Actor with a prompt (here we assume batch_size &#x3D; 1, meaning a single prompt) and let it generate a corresponding response. Then, we feed the “prompt + response” into the “reward-loss” system to calculate the final loss, which will be used to update the Actor.</p>
<h3 id="Reference-Model"><a href="#Reference-Model" class="headerlink" title="Reference Model"></a>Reference Model</h3><p><img src="https://i.dawnlab.me/f063d4393f5842b2071938c7554b821b.png" alt="f063d4393f5842b2071938c7554b821b.png"></p>
<p>The <strong>Reference Model</strong> (abbreviated as <strong>Ref model</strong>) is also initialized using the model from the SFT stage, and its parameters are frozen during training. The primary function of the Ref model is to prevent the Actor model from “deviating.” But how exactly does it do this?</p>
<p>A more detailed explanation of “preventing the model from deviating” is that we want the Actor model to generate human-preference-aligned responses, while also ensuring that its output does not diverge too much from the SFT model. In short, we want the output distributions of both models to be as similar as possible. To measure the similarity of the output distributions, we use the <strong>KL divergence</strong>.</p>
<p>As shown in the diagram:</p>
<ul>
<li>For the Actor model, we feed it a prompt, and it generates the corresponding response. Each token in the response has an associated log probability, which we denote as <code>log_probs</code>.</li>
<li>For the <strong>Ref model</strong>, we feed the <strong>prompt + response</strong> generated by the Actor model, and it also outputs the $log_prob$ for each token, which we call $ref_log_probs$.</li>
</ul>
<p>The similarity between the output distributions of these two models can be measured by $ref_log_probs - log_probs$. This formula can be understood in two ways:</p>
<ul>
<li><p><strong>Intuitively</strong>: If $ref_log_probs$ is higher, it indicates that the <strong>Ref model</strong> is more confident in the output of the <strong>Actor model</strong>. In other words, the <strong>Ref model</strong> also believes that for some state $S_t$, the probability of outputting some action $A_t$ is high $\left(P(A_t \mid S_t)\right)$. In this case, we can say that the Actor model has not deviated from the Ref model.</p>
</li>
<li><p><strong>From the perspective of KL divergence</strong>: In fact PPO has do some changes of KL divergence caculation. <strong>TL; DR</strong>: The Reference Model helps stabilize training by preventing large changes in the Actor’s behavior. We compare the log probabilities of the Actor and Reference Model’s outputs, which is a component used in PPO’s objective function. It’s important to note that this is <em>not</em> the full KL divergence, which is calculated over the entire probability distribution of possible outputs, not just the single chosen output.</p>
</li>
</ul>
<p>In PPO, they often use a <strong>clipped</strong> surrogate objective, which includes this log probability difference, and sometimes a separate KL penalty term. The key is that the KL divergence is calculated over the entire distribution of actions, not just the action chosen by the actor.</p>
<p>To be more precise, the KL divergence between the actor’s policy and the reference policy at a given state s would be:</p>
<p>$$<br>KL(\pi_\theta(\cdot|s) || \pi_{ref}(\cdot|s)) &#x3D; \sum_{a \in A} \pi_\theta(a|s) \log \frac{\pi_\theta(a|s)}{\pi_{ref}(a|s)}<br>$$</p>
<p>Where A is the set of all possible actions (tokens).</p>
<p>Now, we know how to use the Ref model and KL divergence to prevent the Actor from deviating too much. KL divergence will be used in the loss calculation, which will be explained in more detail later.</p>
<h3 id="Critic-Model"><a href="#Critic-Model" class="headerlink" title="Critic Model"></a>Critic Model</h3><p><img src="https://i.dawnlab.me/5087be090f6299634a66cbd932783cae.png" alt="5087be090f6299634a66cbd932783cae.png"></p>
<p>The <strong>Critic Model</strong> is used to predict the expected total return $V_t$, and like the Actor model, it also requires parameter updates. In practice, the design and initialization of the Critic model vary, for example, it might share some parameters with the Actor model or be initialized from the Reward model trained during the RW stage. For consistency, we’ll assume here that it’s initialized from the Reward model of the RW stage.</p>
<p>You might wonder: I understand why we train the Actor model, but why do we need to train a separate Critic model to predict the return?</p>
<p>The reason is that when we discussed the total return $V_t$ (which includes both immediate and future rewards), we assumed an “omniscient perspective,” meaning that $V_t$ is the true, objective total return. However, when training the model, we no longer have this omniscient perspective. At time $t$, we can’t access the true total return $V_t$—instead, we train a model to predict it.</p>
<p>In summary, during RLHF, we need to train the model to generate content that aligns with human preferences (Actor), but we also need to improve the model’s ability to quantify human preferences (Critic). This is the purpose of the Critic model. The architecture of the Critic model is roughly as follows:</p>
<p>The <strong>Critic model</strong> is similar to the Actor model, but with an additional <strong>Value Head</strong> layer at the final stage. This layer is a simple linear layer that maps the raw output to a single value, representing the expected return $V_t$.</p>
<h3 id="Reward-Model"><a href="#Reward-Model" class="headerlink" title="Reward Model"></a>Reward Model</h3><p><img src="https://i.dawnlab.me/01dcb7c4c009c2d672a7c0024c5149c3.png" alt="01dcb7c4c009c2d672a7c0024c5149c3.png"></p>
<p>The <strong>Reward Model</strong> is used to calculate the immediate reward $r_t$ for generating token $a_t$. It is the model trained during the RW stage, and its parameters are frozen during the RLHF phase.</p>
<p>You might ask: Why does the Critic model participate in training, but the Reward model, which is also related to the reward, has frozen parameters?</p>
<p>This is because the Reward model has an “omniscient perspective.” This omniscient perspective has two meanings:</p>
<ol>
<li>The Reward model is trained with data related to “estimated rewards,” so during the RLHF phase, it can be used directly as a model that produces objective values.</li>
<li>The Reward model represents “immediate rewards,” meaning that since token $a_t$ has already been generated, the immediate reward can be computed right away.</li>
</ol>
<p>You might also ask: I already have the Critic model predicting $V_t$, which includes both “immediate” and “future” rewards, so why do I need the Reward model, which only represents the “immediate” reward?</p>
<p>To answer this question, let’s first review the value function:</p>
<p>$V_t &#x3D; R_t + \gamma V_{t+1}$</p>
<p>This function tells us that the total reward at time $t$ can be represented by two results:</p>
<ul>
<li><strong>Result 1</strong>: The value predicted by the <strong>Critic model</strong> $V_t$</li>
<li><strong>Result 2</strong>: The value predicted by the <strong>Reward model</strong> $R_t$ and the <strong>Critic model’s</strong> prediction for the next state $V_{t+1}$</li>
</ul>
<p>Which of these two results is closer to the objective value from the “God’s eye perspective”? Of course, it is <strong>Result 2</strong>, because <strong>Result 1</strong> is purely based on prediction, while $R_t$ in <strong>Result 2</strong> is factual data.</p>
<p>We know that the <strong>Critic model</strong> is also involved in parameter updates. We can use the <strong>Mean Squared Error (MSE)</strong> to measure its loss, where the loss is the difference between the <strong>God’s eye perspective</strong> (the true reward) and the <strong>Critic model’s</strong> predicted value. However, since we do not know the objective reward from the “God’s eye perspective,” we can only approximate it using known factual data. Therefore, we use $R_t + \gamma V_{t+1}$ as an approximation. This is the reason why both $R_t$ and $V_t$ exist together.</p>
<p>The Reward and Critic models are quite similar.</p>
<h3 id="Summary-of-these-part"><a href="#Summary-of-these-part" class="headerlink" title="Summary of these part"></a>Summary of these part</h3><p><img src="https://i.dawnlab.me/2fe1fc912793db3afd36a8823daca1c5.png" alt="2fe1fc912793db3afd36a8823daca1c5.png"></p>
<p>We can use this DeepSeppd Chat workflow to check our understading.</p>
<p>A breif summary:</p>
<p> <strong>Actor Model</strong> (The “Learner”):</p>
<ul>
<li><strong>Purpose</strong>: To generate responses that align with human preferences.<ul>
<li><strong>Data</strong>: It is initialized using the SFT model (Supervised Fine-Tuned model) or other pre-trained models.</li>
<li><strong>Feedback</strong>: The <strong>Actor</strong> is updated using the <strong>reward-loss</strong> computation, which includes feedback from the <strong>Critic</strong> and <strong>Reward models</strong>.</li>
<li><strong>Frozen&#x2F;Updated</strong>: <strong>Updated</strong> during RLHF.</li>
</ul>
</li>
</ul>
<p> <strong>Critic Model</strong> (The “Evaluator”):</p>
<ul>
<li><strong>Purpose</strong>: Predict the total future reward (both immediate and expected rewards) for the sequence generated by the Actor.<ul>
<li><strong>Data</strong>: Typically initialized using the <strong>Reward Model</strong> from a previous phase (RW stage).</li>
<li><strong>Feedback</strong>: The <strong>Critic</strong> is trained based on the rewards the Actor generates, and it predicts the expected return for each response. Its output helps improve the Actor.</li>
<li><strong>Frozen&#x2F;Updated</strong>: <strong>Updated</strong> during RLHF.</li>
</ul>
</li>
</ul>
<p><strong>Reward Model</strong> (The “Immediate Reward Provider”):</p>
<ul>
<li><strong>Purpose</strong>: Provides the <strong>immediate reward</strong> for each token generated by the Actor.</li>
<li><strong>Training</strong>: <ul>
<li><strong>Data</strong>: The Reward Model is trained during the <strong>RW phase</strong>, where it learns to evaluate how well a generated token aligns with human preferences. </li>
<li><strong>Feedback</strong>: After training, the Reward Model gives immediate rewards to the Actor’s actions (tokens) during RLHF. This feedback is used to guide the Actor’s learning.</li>
<li><strong>Frozen&#x2F;Updated</strong>: <strong>Frozen</strong> during RLHF (no updates).</li>
</ul>
</li>
</ul>
<p> <strong>Reference Model</strong> (The “Regulator”):</p>
<ul>
<li><strong>Purpose</strong>: Prevents the Actor from straying too far from the SFT model’s behavior and ensures the output distribution remains similar.</li>
<li><strong>Training</strong>: <ul>
<li><strong>Data</strong>: The Reference Model is also initialized using the <strong>SFT model</strong> and its parameters remain frozen during RLHF.</li>
<li><strong>Feedback</strong>: It compares the output of the Actor with its own output and provides feedback to regulate the Actor’s learning. Specifically, it uses <strong>KL divergence</strong> to measure how similar the distributions are between the Actor and the Reference.</li>
<li><strong>Frozen&#x2F;Updated</strong>: <strong>Frozen</strong> during RLHF (no updates).</li>
</ul>
</li>
</ul>
<hr>
<h3 id="Key-Points"><a href="#Key-Points" class="headerlink" title="Key Points:"></a>Key Points:</h3><ul>
<li><strong>Actor</strong>: <strong>Updated</strong> with feedback from the <strong>Critic</strong> and <strong>Reward Models</strong> during RLHF. It learns to generate responses that are better aligned with human preferences.</li>
<li><strong>Critic</strong>: <strong>Updated</strong> during RLHF to predict the total expected rewards (future returns) for each action the Actor takes.</li>
<li><strong>Reward</strong>: <strong>Frozen</strong> after training during the RW phase and used to provide immediate feedback (rewards) for each token generated by the Actor.</li>
<li><strong>Reference</strong>: <strong>Frozen</strong> during RLHF and used to help ensure the Actor’s output doesn’t diverge too much from the expected distribution of the <strong>SFT model</strong>, typically using <strong>KL divergence</strong>.</li>
</ul>
<h3 id="Go-through-some-concepts"><a href="#Go-through-some-concepts" class="headerlink" title="Go through some concepts"></a>Go through some concepts</h3><p>Let’s go through each comment in this <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/677607581">source</a> to identify which ones are correct and which ones are incorrect:</p>
<ol>
<li><strong>“Reward是对actor模型进行了某一个action之后的直接打分；而critic则是对这个actor模型的整体预估得分。”</strong><ul>
<li><strong>Correct</strong>. <ul>
<li><strong>Reward Model</strong>: Provides immediate feedback for each action taken by the Actor (i.e., a single token generated by the model).</li>
<li><strong>Critic Model</strong>: Estimates the expected future total reward (both immediate and future rewards) for the sequence the Actor generates. The Critic assesses the long-term value of the Actor’s actions, not just the immediate reward.</li>
</ul>
</li>
</ul>
</li>
</ol>
<hr>
<ol start="2">
<li><strong>“每次actor模型更新后，critic模型都要对这个新的actor模型重新打分，所以critic模型也要更新参数。”</strong><ul>
<li><strong>Incorrect</strong>.<ul>
<li><strong>Critic Model</strong> does not need to be updated after every update of the <strong>Actor</strong> model. In fact, in <strong>RLHF</strong> (Reinforcement Learning with Human Feedback), the <strong>Critic</strong> typically <strong>shares parameters</strong> with the <strong>Reward Model</strong> or is initialized from it, and only needs updates during training phases where its performance is evaluated against actual rewards.</li>
<li>The <strong>Critic</strong>‘s role is to provide a value estimate that helps the <strong>Actor</strong> to improve its decision-making. While the <strong>Critic</strong>‘s feedback helps guide the <strong>Actor</strong>, the <strong>Critic</strong> itself may not need to be updated as frequently as the <strong>Actor</strong>.</li>
</ul>
</li>
</ul>
</li>
</ol>
<hr>
<ol start="3">
<li><strong>“critic模型对actor模型的整体预估得分，是根据reward模型的每一次实时打分来预估的。当critic模型的预估得分达到了一定的基准，就代表actor模型训练完成。”</strong><ul>
<li><strong>Partially Correct&#x2F;Incorrect</strong>.<ul>
<li><strong>Correct</strong>: The <strong>Critic Model</strong> does estimate the overall future reward (the value of the entire action sequence) by considering the <strong>Reward Model’s</strong> immediate scores. The <strong>Critic</strong> aggregates the rewards over time, which are influenced by the <strong>Reward Model</strong>.</li>
<li><strong>Incorrect</strong>: The <strong>Critic’s</strong> performance does not directly indicate when the <strong>Actor’s training is complete</strong>. The <strong>Critic</strong> serves to help the <strong>Actor</strong> learn by predicting the expected reward, but the <strong>Actor’s</strong> training completion is not directly tied to the <strong>Critic’s</strong> score reaching a specific threshold. In <strong>RLHF</strong>, the <strong>Actor</strong> continues to improve as long as there is a performance gap between the <strong>generated outputs</strong> and the <strong>desired outputs</strong> (aligned with human preferences).</li>
</ul>
</li>
</ul>
</li>
</ol>
<hr>
<ol start="4">
<li><strong>“我感觉好像reward是一个句子生成结束之后打分，critic是token粒度的打分”</strong><ul>
<li><strong>Incorrect</strong>.<ul>
<li><strong>Reward Model</strong> provides feedback for each token (or action) generated by the <strong>Actor</strong>, not just at the end of the sentence. It’s more about giving immediate rewards for each action taken (each token generated).</li>
<li><strong>Critic Model</strong> provides a value estimate over the entire sequence, not on a token-by-token basis. It predicts the total future reward based on the sequence of tokens produced by the <strong>Actor</strong>, incorporating both immediate and future rewards.</li>
</ul>
</li>
</ul>
</li>
</ol>
<hr>
<ol start="5">
<li><strong>“reward模型，它是在第一阶段sft时，通过人工介入方式训练的一个能量化‘人类偏好’的模型，在PPO中的优势函数，要评估当前动作的好坏（即是否对齐人类偏好），自然就要用到这第一阶段训练到的reward模型来得到这个好坏值。也因此，这个reward网络在此阶段是冻结参数的。”</strong><ul>
<li><strong>Correct</strong>.<ul>
<li>The <strong>Reward Model</strong> is trained in the <strong>SFT</strong> (Supervised Fine-Tuning) phase, where it quantifies <strong>human preferences</strong> by comparing different outputs. This model is then <strong>frozen</strong> during the <strong>RLHF</strong> phase, and the <strong>Actor</strong> uses it to evaluate its actions (tokens) based on how well they align with human preferences.</li>
</ul>
</li>
</ul>
</li>
</ol>
<hr>
<ol start="6">
<li><strong>“最后，就是ref模型，从最基础的概念看，它是无关紧要的，但在rlhf里被引入，只是为了保证actor网络不被训偏（通过kl损失来约束）”</strong><ul>
<li><strong>Correct</strong>.<ul>
<li>The <strong>Reference Model</strong> (Ref Model) is used in <strong>RLHF</strong> primarily to ensure that the <strong>Actor Model</strong> doesn’t diverge too far from the intended behavior (based on the <strong>SFT</strong> model). It does this by penalizing large differences in token generation distributions between the <strong>Actor</strong> and the <strong>Reference Model</strong> using <strong>KL Divergence</strong>.</li>
</ul>
</li>
</ul>
</li>
</ol>

            </div>

            <!-- Post Comments -->
            
    <!-- 使用 DISQUS_CLICK -->
<div id="disqus-comment">
    <div id="disqus_thread"></div>

<!-- add animation -->
<style>
	.disqus_click_btn {
            line-height: 30px;
            margin: 0;
            min-width: 50px;
            padding: 0 14px;
            display: inline-block;
            font-family: "Roboto", "Helvetica", "Arial", sans-serif;
            font-size: 14px;
            font-weight: 400;
            text-transform: uppercase;
            letter-spacing: 0;
            overflow: hidden;
            will-change: box-shadow;
            transition: box-shadow .2s cubic-bezier(.4, 0, 1, 1), background-color .2s cubic-bezier(.4, 0, .2, 1), color .2s cubic-bezier(.4, 0, .2, 1);
            outline: 0;
            cursor: pointer;
            text-decoration: none;
            text-align: center;
            vertical-align: middle;
            border: 0;
            background: rgba(158, 158, 158, .2);
            box-shadow: 0 2px 2px 0 rgba(0, 0, 0, .14), 0 3px 1px -2px rgba(0, 0, 0, .2), 0 1px 5px 0 rgba(0, 0, 0, .12);
            color: #fff;
            background-color: #7EC0EE;
            text-shadow: 0
        }
</style>
	
<div class="btn_click_load" id="disqus_bt"> 
    <button class="disqus_click_btn">点击查看评论</button>
</div>

<!--
<script type="text/javascript">
$('.btn_click_load').click(function() {
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'lee'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    document.getElementById('disqus_bt').style.display = "none";
});
</script>
-->
<script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'https://li_haor.gitee.io/2025/01/22/instruction_and_RLHF/';  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'https://li_haor.gitee.io/2025/01/22/instruction_and_RLHF/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>

<script type="text/javascript">
    $('.btn_click_load').click(function() {  //click to load comments
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document;
            var s = d.createElement('script');
            s.src = '//lee.disqus.com/embed.js';
            s.setAttribute('data-timestamp', + new Date());
            (d.head || d.body).appendChild(s);
        })();
        $('.btn_click_load').css('display','none');
    });
</script>
</div>
<style>
    #disqus-comment{
        background-color: #eee;
        padding: 2pc;
    }
</style>


        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy;Powered By <a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Author  <a target="_blank" rel="noopener" href="https://github.com/haoruilee" style="border-bottom: none;">haoruilee</a></li>
				<li><a target="_blank" rel="noopener" href="https://beian.miit.gov.cn/" style="border-bottom: none;">苏ICP备2020050362号</a></li>
            </ul>
            
				<span id="busuanzi_container_site_pv"> 2025 </span> 
			
        </div>
    </div>
</body>



 	
</html>
