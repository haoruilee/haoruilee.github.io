<!DOCTYPE HTML>
<html>

<head>
	<link rel="bookmark"  type="image/x-icon"  href="/img/logo_miccall.webp"/>
	<link rel="shortcut icon" href="/img/logo_miccall.webp">
	
			    <title>
    Haorui Li
    </title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="/css/mic_main.css" />
    <link rel="stylesheet" href="/css/dropdownMenu.css" />
    <meta name="keywords" content="haoruilee" />
    
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css" />
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('https://i.dawnlab.me/e6a2105d63e4438f288e4ae4d97f53f2.webp') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

			    
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script async type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


    <script src="https://cdnjs.loli.net/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
    <script src="https://cdnjs.loli.net/ajax/libs/jquery.scrollex/0.2.1/jquery.scrollex.min.js"></script>
    <script src="https://cdnjs.loli.net/ajax/libs/skel/3.0.1/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
<meta name="generator" content="Hexo 7.2.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_coy.css" />
<link rel="stylesheet" href="/css/typo.css" />
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="mailto:haoruili@seu.edu.cn" class="logo">Mail Me for offical  or Chat with me for fun</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special" >
            <ul class="menu links" >
			<!-- Homepage  主页  --> 
			<li >
	            <a href="/" rel="nofollow">Home</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <!-- archives  归档   --> 
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/life/" title="CV">
		                CV
		            </a>
		        </li>
		        
		        <li>
		            <a href="/tag/" title="Contact">
		                Contact
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
                    
                    <li>
                        <a title="github" href="https://github.com/haoruilee" target="_blank" rel="noopener">
                            <i class="icon fa fa-github"></i>
                        </a>
                    </li>
                    
			</ul>
</nav>

        <div id="main" >
            <div class ="post_page_title_img" style="height: 25rem;background-image: url();background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;" >
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2 ></h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
                <h1 id="重读AlexNet"><a href="#重读AlexNet" class="headerlink" title="重读AlexNet"></a>重读AlexNet</h1><p><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">https://proceedings.neurips.cc/paper_files&#x2F;paper&#x2F;2012&#x2F;file&#x2F;c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">proceedings.neurips.cc&#x2F;paper_files&#x2F;paper&#x2F;2012&#x2F;file&#x2F;c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</a></p>
<h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><p>After 12 years, reading Alexnet again can still give us insights.</p>
<p>This work comes from UoT, Alex Krizhevsky is first author, his others famous paper:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=xegzhJcAAAAJ&citation_for_view=xegzhJcAAAAJ:9yKSN-GCB0IC">https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=xegzhJcAAAAJ&amp;citation_for_view=xegzhJcAAAAJ:9yKSN-GCB0IC</a></li>
<li><a target="_blank" rel="noopener" href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=xegzhJcAAAAJ&citation_for_view=xegzhJcAAAAJ:d1gkVwhDpl0C">https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=xegzhJcAAAAJ&amp;citation_for_view=xegzhJcAAAAJ:d1gkVwhDpl0C</a></li>
<li><a target="_blank" rel="noopener" href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=xegzhJcAAAAJ&citation_for_view=xegzhJcAAAAJ:u-x6o8ySG0sC">https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=xegzhJcAAAAJ&amp;citation_for_view=xegzhJcAAAAJ:u-x6o8ySG0sC</a></li>
</ul>
<h1 id="Abstract-Part"><a href="#Abstract-Part" class="headerlink" title="Abstract Part"></a>Abstract Part</h1><p>Abstract part doesn’t really explain the network or any algorithms, but it points out AlexNet (In paper called ‘our network’) is much more better than the rank2 method in ImageNet competition, and it is a network with numerous params and trained on 2 GPUs, which are novel at 2012.</p>
<h1 id="Conclusion-Part"><a href="#Conclusion-Part" class="headerlink" title="Conclusion Part"></a>Conclusion Part</h1><p>AlextNet paper doesnt have a namely ‘Conclusion’, instead it has a ‘Discussion’. These discussions are really smart even seen at today:</p>
<ul>
<li>Our results show that a large, deep convolutional neural network is capable of achieving recordbreaking results on a highly challenging dataset using purely supervised learning.</li>
</ul>
<p>This start a new subject called ‘Deep Learning’. And finally we got ‘Scaling Law’ which bring us LLMs.</p>
<p>- </p>
<pre><code>It is notable that our network’s performance degrades if a single convolutional layer is removed. For example, removing any of the middle layers results in a loss of about 2% for the top-1 performance of the network. So the depth really is important for achieving our results.
</code></pre>
<p>But this part is a little bit not making sense, cuz you compare different network’s depth but didnt control their params, some how you can still reach better results if you have larger ‘width’ but smaller ‘depth’.</p>
<p>- </p>
<pre><code>To simplify our experiments, we did not use any unsupervised pre-training even though we expect that it will help, especially if we obtain enough computational power to significantly increase the size of the network without obtaining a corresponding increase in the amount of labeled data.
</code></pre>
<p>As we know scaling law now, this is not really true, after the model become much more larger, without significant amount of data, you will finally reach threshold.</p>
<p>What’s more. after 12 years we finally find it is really hard to get large amount of high quality labeled data, so unsupervised learning is relatively more important in current machine learning area.</p>
<h1 id="Architecture-Part"><a href="#Architecture-Part" class="headerlink" title="Architecture Part"></a>Architecture Part</h1><p>Architecture part first point out three novelty of this work:</p>
<ul>
<li>ReLU Nonlinearity</li>
<li>Train on multiple GPUs</li>
<li>Local Response Normalization</li>
<li>Overlapping Pooling</li>
</ul>
<h2 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h2><p>At that point, the main track of nn uses tanh and sigmoid. Alex use ReLU which is proposed from Abien Fred M. Agarap</p>
<p>sigmoid: $f(x) &#x3D; 1 &#x2F; {1 - e^{-x}}$  In deep networks, the gradients of the sigmoid function can become very small, especially for neurons in the early layers. </p>
<p>tanh: $f(x) &#x3D; \frac{e^x - e^{-x}} {e^x + e^{-x}}$. The gradients are larger compared to the sigmoid function, which can help with the learning process.</p>
<p>Rectified Linear Units(ReLU): $f(x) &#x3D; max(0, x)$. ReLU does not suffer from the vanishing gradient problem. Gradients are preserved better during backpropagation, which helps in training deep networks. ReLU introduces sparsity in the network by outputting zero for negative input values, which can lead to more efficient and faster computations. Also it is really easy to differentiate.</p>
<h2 id="Train-on-multiple-GPUs"><a href="#Train-on-multiple-GPUs" class="headerlink" title="Train on multiple GPUs"></a>Train on multiple GPUs</h2><p>Nowadays we call it ‘Distributed Training’, torch.distributed give us more efficient tools nowadays.</p>
<h2 id="Local-Response-Normalization"><a href="#Local-Response-Normalization" class="headerlink" title="Local Response Normalization"></a>Local Response Normalization</h2><p>This part AlexNet design a complex function to do normalization, seems he want to add spatial messages to normalization, but now we know it is not very important to do normalization, but it is important to design a good network. And we have relatively better normalization nowadays like epoch  normalization and layer normalization.</p>
<h2 id="Overlapping-Pooling"><a href="#Overlapping-Pooling" class="headerlink" title="Overlapping Pooling"></a>Overlapping Pooling</h2><p> This part this a tick, that AlexNet have overlapped pooling layers.</p>
<h2 id="Overall-Architecture"><a href="#Overall-Architecture" class="headerlink" title="Overall Architecture"></a>Overall Architecture</h2><p><img src="/%E9%87%8D%E8%AF%BBAlexNet%20d8c02783f910445ba86a9c9702660af8/Untitled.png" alt="Untitled"></p>
<p>AlexNet is a type of CNN.</p>
<ul>
<li>The first convolutional layer filters the 224×224×3 input image with 96 kernels of size 11×11×3<br>with a stride of 4 pixels. (And with 2 padding)</li>
</ul>
<p>$\text { Output Size }&#x3D;\left\lfloor\frac{(\text { Input Size }- \text { Filter Size }+2 \times \text { Padding })}{\text { Stride }}\right\rfloor+1$</p>
<p>So it will be:</p>
<p>$\text { Output Size }&#x3D;\left\lfloor\frac{(\text { 224 }- \text { 11 }+2 \times \text { 2 })}{\text { 4 }}\right\rfloor+1 &#x3D; \lfloor \frac{217}{4} \rfloor +1 &#x3D;  54+1 &#x3D; 55$</p>
<p>For the depth, it has two separate image so the depth is $\frac{96}{2} &#x3D; {48}$</p>
<p>So we have:</p>
<ul>
<li>The second convolutional layer takes as input the (response-normalized and pooled) output of the first convolutional layer and filters it with 256 kernels of size 5 × 5 × 48.</li>
</ul>
<p>Also need to mark:</p>
<ul>
<li>The third convolutional layer has 384 kernels of size 3 × 3 ×<br>256 connected to the (normalized, pooled) outputs of the second convolutional layer.</li>
</ul>
<p>Here AlexNet make the convolution results of second layer to both third layer, that means “The GPUs communicate only at certain layers.”</p>
<ul>
<li>The fully-connected layers have 4096 neurons each.</li>
</ul>
<h1 id="Data-Part"><a href="#Data-Part" class="headerlink" title="Data Part"></a>Data Part</h1><h2 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h2><ul>
<li>The first form of data augmentation consists of generating image translations and horizontal reflections. We do this by extracting random 224 × 224 patches (and their horizontal reflections) from the 256×256 images and training our network on these extracted patches</li>
</ul>
<p>This doest really make sense cuz the new image are almost the same.</p>
<ul>
<li>The second form of data augmentation consists of altering the intensities of the RGB channels in<br>training images.</li>
</ul>
<p>But in fact at the dataset part AlexNet write ”So we trained our network on the (centered) raw RGB values of the pixels.” So perhaps this hasnt use.</p>
<h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>At that time scientists think with drop out we can get several different models. but now we know it is same to L2 norm.</p>
<p>Dropout: A Simple Way to Prevent Neural Networks from Overfitting. By Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. UoT.</p>
<h1 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h1><p>The results are unprecedented good. But what is really worth thinking is the 6.1 Quotative Evaluation part.</p>
<p><img src="/%E9%87%8D%E8%AF%BBAlexNet%20d8c02783f910445ba86a9c9702660af8/Untitled%201.png" alt="Untitled"></p>
<ul>
<li>Five ILSVRC-2010 test images in the first column. The remaining columns show the six training images that produce feature vectors in the last hidden layer with the smallest Euclidean distance from the feature vector for the test image.</li>
</ul>
<p>This shows that CNN is really good at image embedding.</p>

            </div>

            <!-- Post Comments -->
            
    <!-- 使用 DISQUS_CLICK -->
<div id="disqus-comment">
    <div id="disqus_thread"></div>

<!-- add animation -->
<style>
	.disqus_click_btn {
            line-height: 30px;
            margin: 0;
            min-width: 50px;
            padding: 0 14px;
            display: inline-block;
            font-family: "Roboto", "Helvetica", "Arial", sans-serif;
            font-size: 14px;
            font-weight: 400;
            text-transform: uppercase;
            letter-spacing: 0;
            overflow: hidden;
            will-change: box-shadow;
            transition: box-shadow .2s cubic-bezier(.4, 0, 1, 1), background-color .2s cubic-bezier(.4, 0, .2, 1), color .2s cubic-bezier(.4, 0, .2, 1);
            outline: 0;
            cursor: pointer;
            text-decoration: none;
            text-align: center;
            vertical-align: middle;
            border: 0;
            background: rgba(158, 158, 158, .2);
            box-shadow: 0 2px 2px 0 rgba(0, 0, 0, .14), 0 3px 1px -2px rgba(0, 0, 0, .2), 0 1px 5px 0 rgba(0, 0, 0, .12);
            color: #fff;
            background-color: #7EC0EE;
            text-shadow: 0
        }
</style>
	
<div class="btn_click_load" id="disqus_bt"> 
    <button class="disqus_click_btn">点击查看评论</button>
</div>

<!--
<script type="text/javascript">
$('.btn_click_load').click(function() {
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'lee'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    document.getElementById('disqus_bt').style.display = "none";
});
</script>
-->
<script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'https://li_haor.gitee.io/proj/raed-alexnet-in-2024.html';  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'https://li_haor.gitee.io/proj/raed-alexnet-in-2024.html'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>

<script type="text/javascript">
    $('.btn_click_load').click(function() {  //click to load comments
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document;
            var s = d.createElement('script');
            s.src = '//lee.disqus.com/embed.js';
            s.setAttribute('data-timestamp', + new Date());
            (d.head || d.body).appendChild(s);
        })();
        $('.btn_click_load').css('display','none');
    });
</script>
</div>
<style>
    #disqus-comment{
        background-color: #eee;
        padding: 2pc;
    }
</style>


        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy;Powered By <a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Author  <a target="_blank" rel="noopener" href="https://github.com/haoruilee" style="border-bottom: none;">haoruilee</a></li>
				<li><a target="_blank" rel="noopener" href="https://beian.miit.gov.cn/" style="border-bottom: none;">苏ICP备2020050362号</a></li>
            </ul>
            
				<span id="busuanzi_container_site_pv"> 2025 </span> 
			
        </div>
    </div>
</body>



 	
</html>
